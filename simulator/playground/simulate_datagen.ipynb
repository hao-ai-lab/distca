{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Fake Tensor\n",
    "\n",
    "- attn_time\n",
    "- mlp_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate synthetic attention time data\n",
    "# Shape: (tp_degree_log, cp_degree_log, seq_len_log)\n",
    "# tp_degree_log: [1,2,4,8] -> 4 values \n",
    "# cp_degree_log: [1,2,4,8] -> 4 values\n",
    "# seq_len_log: [2^10, 2^11, ..., 2^20] -> 11 values\n",
    "tp_degrees = [0, 1, 2, 3] # log2 of [1,2,4,8]\n",
    "cp_degrees = [0, 1, 2, 3] # log2 of [1,2,4,8] \n",
    "seq_lens = np.arange(10, 21) # log2 of sequence lengths\n",
    "\n",
    "attn_time = np.zeros((len(tp_degrees), len(cp_degrees), len(seq_lens)))\n",
    "\n",
    "# Fill with synthetic data that follows these patterns:\n",
    "# 1. Larger sequence lengths take more time\n",
    "# 2. More parallelism (higher tp/cp) reduces time, but with diminishing returns\n",
    "for i, tp in enumerate(tp_degrees):\n",
    "    for j, cp in enumerate(cp_degrees):\n",
    "        for k, seq_len in enumerate(seq_lens):\n",
    "            # Base computation scales quadratically with sequence length\n",
    "            base_time = 2**(2*seq_len - 20) # Normalize to make numbers reasonable\n",
    "            \n",
    "            # Parallelism benefit with diminishing returns\n",
    "            parallel_factor = 1.0 / ((2**tp + 2**cp)**0.8)\n",
    "            \n",
    "            attn_time[i,j,k] = base_time * parallel_factor\n",
    "\n",
    "# Generate synthetic MLP time data  \n",
    "# Shape: (tp_degree_log, num_token_log)\n",
    "mlp_time = np.zeros((len(tp_degrees), len(seq_lens)))\n",
    "\n",
    "for i, tp in enumerate(tp_degrees):\n",
    "    for j, seq_len in enumerate(seq_lens):\n",
    "        # MLP computation scales linearly with sequence length\n",
    "        base_time = 2**(seq_len - 10) # Normalize\n",
    "        \n",
    "        # TP parallelism benefit\n",
    "        parallel_factor = 1.0 / (2**tp)**0.9\n",
    "        \n",
    "        mlp_time[i,j] = base_time * parallel_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[5.74349177e-01, 2.29739671e+00, 9.18958684e+00, 3.67583474e+01,\n",
       "         1.47033389e+02, 5.88133558e+02, 2.35253423e+03, 9.41013692e+03,\n",
       "         3.76405477e+04, 1.50562191e+05, 6.02248763e+05],\n",
       "        [4.15243647e-01, 1.66097459e+00, 6.64389834e+00, 2.65755934e+01,\n",
       "         1.06302374e+02, 4.25209494e+02, 1.70083798e+03, 6.80335190e+03,\n",
       "         2.72134076e+04, 1.08853630e+05, 4.35414522e+05],\n",
       "        [2.75945932e-01, 1.10378373e+00, 4.41513492e+00, 1.76605397e+01,\n",
       "         7.06421587e+01, 2.82568635e+02, 1.13027454e+03, 4.52109815e+03,\n",
       "         1.80843926e+04, 7.23375705e+04, 2.89350282e+05],\n",
       "        [1.72427286e-01, 6.89709144e-01, 2.75883658e+00, 1.10353463e+01,\n",
       "         4.41413852e+01, 1.76565541e+02, 7.06262163e+02, 2.82504865e+03,\n",
       "         1.13001946e+04, 4.52007785e+04, 1.80803114e+05]],\n",
       "\n",
       "       [[4.15243647e-01, 1.66097459e+00, 6.64389834e+00, 2.65755934e+01,\n",
       "         1.06302374e+02, 4.25209494e+02, 1.70083798e+03, 6.80335190e+03,\n",
       "         2.72134076e+04, 1.08853630e+05, 4.35414522e+05],\n",
       "        [3.29876978e-01, 1.31950791e+00, 5.27803164e+00, 2.11121266e+01,\n",
       "         8.44485063e+01, 3.37794025e+02, 1.35117610e+03, 5.40470440e+03,\n",
       "         2.16188176e+04, 8.64752704e+04, 3.45901082e+05],\n",
       "        [2.38494847e-01, 9.53979387e-01, 3.81591755e+00, 1.52636702e+01,\n",
       "         6.10546808e+01, 2.44218723e+02, 9.76874893e+02, 3.90749957e+03,\n",
       "         1.56299983e+04, 6.25199931e+04, 2.50079973e+05],\n",
       "        [1.58489319e-01, 6.33957277e-01, 2.53582911e+00, 1.01433164e+01,\n",
       "         4.05732657e+01, 1.62293063e+02, 6.49172252e+02, 2.59668901e+03,\n",
       "         1.03867560e+04, 4.15470241e+04, 1.66188096e+05]],\n",
       "\n",
       "       [[2.75945932e-01, 1.10378373e+00, 4.41513492e+00, 1.76605397e+01,\n",
       "         7.06421587e+01, 2.82568635e+02, 1.13027454e+03, 4.52109815e+03,\n",
       "         1.80843926e+04, 7.23375705e+04, 2.89350282e+05],\n",
       "        [2.38494847e-01, 9.53979387e-01, 3.81591755e+00, 1.52636702e+01,\n",
       "         6.10546808e+01, 2.44218723e+02, 9.76874893e+02, 3.90749957e+03,\n",
       "         1.56299983e+04, 6.25199931e+04, 2.50079973e+05],\n",
       "        [1.89464571e-01, 7.57858283e-01, 3.03143313e+00, 1.21257325e+01,\n",
       "         4.85029301e+01, 1.94011721e+02, 7.76046882e+02, 3.10418753e+03,\n",
       "         1.24167501e+04, 4.96670005e+04, 1.98668002e+05],\n",
       "        [1.36979319e-01, 5.47917277e-01, 2.19166911e+00, 8.76667642e+00,\n",
       "         3.50667057e+01, 1.40266823e+02, 5.61067291e+02, 2.24426916e+03,\n",
       "         8.97707666e+03, 3.59083066e+04, 1.43633227e+05]],\n",
       "\n",
       "       [[1.72427286e-01, 6.89709144e-01, 2.75883658e+00, 1.10353463e+01,\n",
       "         4.41413852e+01, 1.76565541e+02, 7.06262163e+02, 2.82504865e+03,\n",
       "         1.13001946e+04, 4.52007785e+04, 1.80803114e+05],\n",
       "        [1.58489319e-01, 6.33957277e-01, 2.53582911e+00, 1.01433164e+01,\n",
       "         4.05732657e+01, 1.62293063e+02, 6.49172252e+02, 2.59668901e+03,\n",
       "         1.03867560e+04, 4.15470241e+04, 1.66188096e+05],\n",
       "        [1.36979319e-01, 5.47917277e-01, 2.19166911e+00, 8.76667642e+00,\n",
       "         3.50667057e+01, 1.40266823e+02, 5.61067291e+02, 2.24426916e+03,\n",
       "         8.97707666e+03, 3.59083066e+04, 1.43633227e+05],\n",
       "        [1.08818820e-01, 4.35275282e-01, 1.74110113e+00, 6.96440451e+00,\n",
       "         2.78576180e+01, 1.11430472e+02, 4.45721888e+02, 1.78288755e+03,\n",
       "         7.13155021e+03, 2.85262009e+04, 1.14104803e+05]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated synthetic timing data:\n",
      "attn_time shape: (4, 4, 11)\n",
      "mlp_time shape: (4, 11)\n",
      "\n",
      "Sample attn_time values:\n",
      " [  0.57434918   2.29739671   9.18958684  36.75834736 147.03338944]\n",
      "\n",
      "Sample mlp_time values:\n",
      " [ 1.  2.  4.  8. 16.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save to files\n",
    "np.save('attn_time.npy', attn_time)\n",
    "np.save('mlp_time.npy', mlp_time)\n",
    "\n",
    "print(\"Generated synthetic timing data:\")\n",
    "print(\"attn_time shape:\", attn_time.shape)\n",
    "print(\"mlp_time shape:\", mlp_time.shape)\n",
    "print(\"\\nSample attn_time values:\\n\", attn_time[0,0,:5])\n",
    "print(\"\\nSample mlp_time values:\\n\", mlp_time[0,:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(data_path=None, tokenizer=None, batch_size=32, tp_degree=1, cp_degree=1, dp_degree=1, num_tokens_per_data=512, attn_time='attn_time.npy', mlp_time='mlp_time.npy', batch_samples=100)\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/None/resolve/main/tokenizer_config.json\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/transformers/utils/hub.py\", line 424, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1008, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1115, in _hf_hub_download_to_cache_dir\n",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1643, in _raise_on_head_call_error\n",
      "    raise head_call_error\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1531, in _get_metadata_or_catch_error\n",
      "    metadata = get_hf_file_metadata(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1448, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 286, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 310, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 459, in hf_raise_for_status\n",
      "    raise _format(RepositoryNotFoundError, message, response) from e\n",
      "huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-68212e3f-1338c6d325f1bb7815e19910;5e193450-4924-4d6f-8643-b74e15142921)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/None/resolve/main/tokenizer_config.json.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/pscratch/sd/j/jundac/project/d2/simulator.py\", line 347, in <module>\n",
      "    main()\n",
      "  File \"/pscratch/sd/j/jundac/project/d2/simulator.py\", line 334, in main\n",
      "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py\", line 946, in from_pretrained\n",
      "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py\", line 778, in get_tokenizer_config\n",
      "    resolved_config_file = cached_file(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/transformers/utils/hub.py\", line 266, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/global/homes/j/jundac/envs/d2/lib/python3.12/site-packages/transformers/utils/hub.py\", line 456, in cached_files\n",
      "    raise OSError(\n",
      "OSError: None is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
     ]
    }
   ],
   "source": [
    "!python simulator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Generation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# data_path = \"wikitext\"\n",
    "# \t`load_dataset('wikitext', 'wikitext-103-raw-v1')`\n",
    "doc_dataset = load_dataset(\"wikitext\", 'wikitext-103-raw-v1', streaming=True)\n",
    "doc_dataset = doc_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(num_tokens_per_data, tokenizer, doc_dataset):\n",
    "    data_budget = num_tokens_per_data\n",
    "    doc_lens = []\n",
    "    for data in doc_dataset:\n",
    "        text = data[\"text\"]\n",
    "        tokenized_text = tokenizer(text)\n",
    "        token_count = len(tokenized_text[\"input_ids\"])\n",
    "        if token_count == 0:\n",
    "            continue\n",
    "        while data_budget < token_count:\n",
    "            doc_lens.append(data_budget)\n",
    "            yield doc_lens\n",
    "            doc_lens = []\n",
    "            data_budget = num_tokens_per_data\n",
    "            token_count -= data_budget\n",
    "\n",
    "        if data_budget >= token_count:\n",
    "            doc_lens.append(token_count)\n",
    "            data_budget -= token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = get_data(2 ** (10 + 4), tokenizer, doc_dataset)\n",
    "data = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 183, 188, 7, 11, 152, 218, 11, 206, 247, 8, 9, 9, 9, 6, 113, 123, 7, 192, 10, 98, 61, 64, 134, 68, 143, 179, 68, 9, 199, 29, 101, 180, 26, 62, 10, 188, 10, 183, 87, 50, 69, 7, 28, 9, 114, 71, 10, 110, 77, 84, 10, 269, 116, 37, 83, 7, 78, 97, 138, 250, 137, 71, 12, 82, 358, 299, 7, 228, 129, 36, 11, 148, 151, 130, 52, 9, 165, 161, 10, 218, 226, 8, 252, 240, 8, 96, 201, 142, 140, 8, 251, 8, 4, 10, 10, 9, 17, 10, 9, 12, 8, 11, 6, 17, 13, 8, 8, 8, 12, 140, 224, 7, 105, 214, 150, 171, 9, 419, 136, 185, 10, 424, 233, 10, 192, 196, 390, 205, 276, 9, 240, 212, 12, 288, 9, 148, 10, 255, 11, 220, 11, 242, 231, 79, 10, 166, 181, 7, 7, 374, 11, 112, 181, 92, 74]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doclen_avg: [6250, 12500, 18750, 25000, 31250, 37500, 43750, 50000, 56250, 62500, 68750, 75000, 81250, 87500, 93750, 100000, 106250, 112500, 118750, 125000]\n",
      "doclen_cnt: [96784, 1200, 350, 170, 110, 100, 80, 70, 60, 50, 40, 70, 60, 30, 80, 70, 40, 20, 10, 125]\n"
     ]
    }
   ],
   "source": [
    "# 50000//8\n",
    "doclen_cnt = [\n",
    "    96784, 1200, 350, 170, 110, 100, 80, 70, \n",
    "    60, 50 ,40 ,70, 60,30,80,70,40,20, 10,125\n",
    "]\n",
    "doclen_avg = [\n",
    "    50000//8 * (i + 1)\n",
    "    for i in range(len(doclen_cnt))\n",
    "]\n",
    "print(f\"doclen_avg: {doclen_avg}\")\n",
    "print(f\"doclen_cnt: {doclen_cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# set a seed\n",
    "random.seed(42)\n",
    "dataset = []\n",
    "scope = 50000//8\n",
    "for i in range(len(doclen_cnt)):\n",
    "    for j in range(doclen_cnt[i]):\n",
    "        k = random.randint(i * scope + 1, (i+1) * scope)\n",
    "        dataset.append(k)\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2792, 4238, 642, 5298, 2888, 6033, 3408, 4108, 3978, 3404]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "current_batch = []\n",
    "K = 1024\n",
    "context_length = 16 * K\n",
    "for i in dataset:\n",
    "    if sum(current_batch) + i > context_length:\n",
    "        batches.append(current_batch)\n",
    "        current_batch = []\n",
    "    current_batch.append(i)\n",
    "if current_batch:\n",
    "    batches.append(current_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24221"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15858\n"
     ]
    }
   ],
   "source": [
    "print(sum(batches[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('batches.json', 'w') as f:\n",
    "    json.dump(batches, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
