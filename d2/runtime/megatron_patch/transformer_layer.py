from contextlib import contextmanager, nullcontext
import functools
from typing import Any, Dict, List, Optional, Union
import types
import warnings
import time
import torch
from torch import Tensor

from megatron.core import tensor_parallel, parallel_state
from megatron.core.inference.contexts import BaseInferenceContext
from megatron.core.models.gpt.gpt_model import GPTModel
from megatron.core.transformer.transformer_block import (
    TransformerBlock as MegatronTransformerBlock
)
from megatron.core.utils import WrappedTensor, make_viewless_tensor

from d2.runtime.megatron_patch.fused_comm_attn import FlashAttnArgs, FusedCommAttn, dummy_backward, post_a2a_attn_out_with_lse
from d2.runtime.megatron_patch.base_transformer_layer import TransformerLayer as BaseTransformerLayer
from d2.runtime.megatron_patch.packed_seq_params import PingPangPackedSeqParams, PingPangSingleStepPackedSeqParams
from d2.runtime.megatron_patch.stream_sync_fn import TickSync
from d2.runtime.fast_dispatch_fn import (
    all_to_all, post_all2all_layout_transfer, pre_all2all_layout_transfer
)


#### Tool functions for splitting and gathering args ####
def _split_tensor(x: Optional[torch.Tensor], num_splits: int):
    if x is None:
        return (None,) * num_splits
    return x.split(x.shape[0] // num_splits, dim=0)

def _repack_args(args: List[List[torch.Tensor]], num_splits: int):
    assert all(len(a) == num_splits for a in args)
    return [
        [a[i] for a in args]
        for i in range(num_splits)
    ]

def _repack_dicts(args: Dict[str, List[torch.Tensor]], num_splits: int):
    assert all(len(a) == num_splits for a in args.values())
    return [
        {k: a[i] for k, a in args.items()}
        for i in range(num_splits)
    ]

def _splits_all(tensors: List[torch.Tensor], num_splits: int):
    splits = [_split_tensor(t, num_splits) for t in tensors]
    return _repack_args(splits, num_splits)

def _split_all_dict(tensors: Dict[str, torch.Tensor], num_splits: int):
    splits = {k: _split_tensor(v, num_splits) for k, v in tensors.items()}
    return _repack_dicts(splits, num_splits)

def _gather_tensor(tensors: List[torch.Tensor], num_splits: int):
    assert len(tensors) == num_splits
    if any(t is None for t in tensors):
        assert all(t is None for t in tensors), "None tensors in gather_tensor"
        return None
    return torch.cat(tensors, dim=0)
####

stack = []
def nvtx_range_push(name: str):
    stack.append(name)
    torch.cuda.nvtx.range_push(name)

def nvtx_range_pop(name: str):
    assert stack[-1] == name, f"stack = {stack}"
    stack.pop()
    torch.cuda.nvtx.range_pop()

class TransformerLayer(BaseTransformerLayer):
    ########## Attention Layout <-> MLP Layout Transformation ##########
    def _pre_attn_to_mlp(
        self, attn_out: torch.Tensor,
        packed_seq_params: PingPangSingleStepPackedSeqParams,
    ):
        """
        Communication between attention layout and mlp layout.
        This operation runs on the stream `packed_seq_params.stream`.
        """
        torch.cuda.nvtx.range_push("pre_attn_to_mlp")
        num_heads = self.config.num_attention_heads
        head_dim = self.config.hidden_size // self.config.num_attention_heads
        num_heads_local = num_heads // self.config.tensor_model_parallel_size
        attn_out = attn_out.view(attn_out.shape[0], num_heads_local * head_dim)
        ####
        signal = pre_all2all_layout_transfer.apply(
            attn_out, None, None,
            packed_seq_params.attn_out_fwd_metadata,
            packed_seq_params.attn_out_bwd_metadata,
            packed_seq_params.dispatcher_id,
            False,  # is_qkv
        )
        torch.cuda.nvtx.range_pop()
        return signal

    def _post_attn_to_mlp(
        self, signal: torch.Tensor,
        packed_seq_params: PingPangSingleStepPackedSeqParams,
    ):
        num_heads = self.config.num_attention_heads
        head_dim = self.config.hidden_size // num_heads
        tp_size = self.config.tensor_model_parallel_size
        num_heads_local = num_heads // tp_size
        torch.cuda.nvtx.range_push("post_attn_to_mlp")
        attn_out = post_all2all_layout_transfer.apply(
            signal,
            packed_seq_params.attn_out_fwd_metadata,
            packed_seq_params.attn_out_bwd_metadata,
            packed_seq_params.dispatcher_id,
            False,  # is_qkv
        )
        attn_out = attn_out.reshape(
            attn_out.shape[0], 1, num_heads_local * head_dim,
        ).contiguous()
        torch.cuda.nvtx.range_pop()
        return attn_out

    def _pre_mlp_to_attn(
        self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
        packed_seq_params: PingPangSingleStepPackedSeqParams,
    ):
        """
        Communication between attention layout and mlp layout.
        This operation runs on the stream `packed_seq_params.stream`.
        """

        torch.cuda.nvtx.range_push("mlp_to_attn")
        # TODO(yonghao): current we do a walk around: merge head dimension
        # into hidden dimension. In this way, we need the TP degree being
        # kept for attention and other parts.
        assert query.dim() in [3, 4], f"{query.shape=}, should be t1nh or tnh layout"
        num_q_heads = self.config.num_attention_heads
        num_kv_heads = self.config.num_query_groups or self.config.num_attention_heads
        head_dim = self.config.hidden_size // num_q_heads
        tp_size = self.config.tensor_model_parallel_size
        # TODO: write memcpy to nvshmem buffer kernel with stride support.
        query = query.reshape(query.shape[0], num_q_heads * head_dim // tp_size)
        key = key.reshape(key.shape[0], num_kv_heads * head_dim // tp_size)
        value = value.reshape(value.shape[0], num_kv_heads * head_dim // tp_size)
        ####
        signal = pre_all2all_layout_transfer.apply(
            query, key, value,
            packed_seq_params.qkv_fwd_metadata, # query_metadata
            packed_seq_params.qkv_bwd_metadata, # rev_query_metadata
            packed_seq_params.dispatcher_id,
            True,  # is_qkv
        )
        torch.cuda.nvtx.range_pop()

        return signal

    def _post_mlp_to_attn(
        self, signal: torch.Tensor,
        packed_seq_params: PingPangSingleStepPackedSeqParams,
    ):
        num_q_heads = self.config.num_attention_heads
        num_kv_heads = self.config.num_query_groups or self.config.num_attention_heads
        head_dim = self.config.hidden_size // num_q_heads

        tp_size = self.config.tensor_model_parallel_size
        num_q_heads_local = num_q_heads // tp_size
        num_kv_heads_local = num_kv_heads // tp_size
        torch.cuda.nvtx.range_push("post_mlp_to_attn")
        query, key, value = post_all2all_layout_transfer.apply(
            signal,
            packed_seq_params.qkv_fwd_metadata,
            packed_seq_params.qkv_bwd_metadata,
            packed_seq_params.dispatcher_id,
            True,  # is_qkv
        )
        query = query.view(query.shape[0], num_q_heads_local, head_dim).contiguous()
        key = key.view(key.shape[0], num_kv_heads_local, head_dim).contiguous()
        value = value.view(value.shape[0], num_kv_heads_local, head_dim).contiguous()
        torch.cuda.nvtx.range_pop()
        return query, key, value

    def _all_to_all(self, signal: torch.Tensor,
                    packed_seq_params: PingPangSingleStepPackedSeqParams,
                    is_qkv: bool,):
        metadatas = []
        if is_qkv:
            metadatas = [
                packed_seq_params.qkv_fwd_metadata,
                packed_seq_params.qkv_bwd_metadata,
            ]
        else:
            metadatas = [
                packed_seq_params.attn_out_fwd_metadata,
                packed_seq_params.attn_out_bwd_metadata,
            ]
        signal = all_to_all.apply(
            signal,
            *metadatas,
            packed_seq_params.dispatcher_id,
            packed_seq_params.stream,
        )
        return signal

    ########## Ping-Pong ##########
    def ping_pang_forward(
        self,
        hidden_states: Tensor,
        attention_mask: Optional[Tensor] = None,
        context: Optional[Tensor] = None,
        context_mask: Optional[Tensor] = None,
        rotary_pos_emb: Optional[Tensor] = None,
        rotary_pos_cos: Optional[Tensor] = None,
        rotary_pos_sin: Optional[Tensor] = None,
        attention_bias: Optional[Tensor] = None,
        inference_context: Optional[Any] = None,
        packed_seq_params: Optional[PingPangPackedSeqParams] = None,
        sequence_len_offset: Optional[Tensor] = None,
        *,
        inference_params: Optional[Any] = None,
    ):
        """
        In-place Ping-Pang parallel
        """
        assert inference_params is None, "inference not supported yet"
        assert inference_context is None, "inference not supported yet"
        assert context is None, "cross-attention not supported yet"
        assert context_mask is None, "cross-attention not supported yet"

        debug = packed_seq_params.debug
        # NOTE: transformer_block.py sets layer_number to 1 for the first layer
        needs_split = debug or self.layer_number == 1
        needs_gather = debug or packed_seq_params.do_gather   # NOTE: cannot infer if this is the last local layer or not.

        packed_seq_params_0 = packed_seq_params.seq_params[0]
        packed_seq_params_1 = packed_seq_params.seq_params[1]
        mlp_packed_seq_params_0 = packed_seq_params.mlp_layout_seq_params[0]
        mlp_packed_seq_params_1 = packed_seq_params.mlp_layout_seq_params[1]
        compute_stream = torch.cuda.current_stream()
        if debug:
            setattr(packed_seq_params_0, "stream", compute_stream)
            setattr(packed_seq_params_1, "stream", compute_stream)
        else:
            setattr(packed_seq_params_0, "dispatcher_id", 0)
            setattr(packed_seq_params_1, "dispatcher_id", 1)


        # NOTE: DO NOT REMOVE THIS DEBUG TENSOR! Torch seems to have some
        # liveness issue, that deallocates the tensors too early before
        # communication kernels on the customized stream is done.
        # This list help increasing the tensor's liveness a bit.
        debug_tensors = None
        # 1. split input into two microbatches
        args = [hidden_states, attention_mask, context, context_mask, rotary_pos_emb,
                rotary_pos_cos, rotary_pos_sin, attention_bias, sequence_len_offset]
        if needs_split:
            args_0, args_1 = _splits_all(args, 2)
        else:
            args_0, args_1 = _repack_args(args, 2)
        (hidden_states_0, attention_mask_0, context_0, context_mask_0, rotary_pos_emb_0,
            rotary_pos_cos_0, rotary_pos_sin_0, attention_bias_0, sequence_len_offset_0) = args_0
        (hidden_states_1, attention_mask_1, context_1, context_mask_1, rotary_pos_emb_1,
            rotary_pos_cos_1, rotary_pos_sin_1, attention_bias_1, sequence_len_offset_1) = args_1

        comm_stream = packed_seq_params_0.stream
        assert comm_stream.stream_id == packed_seq_params_1.stream.stream_id

        # 2. pre-self-attention forward microbatch 0.
        ## compute,0
        torch.cuda.nvtx.range_push("pre_core_attn.0")
        query_0, key_0, value_0, residual_0, attn_mask_type_0 = self._forward_pre_core_attn(
            hidden_states_0,
            rotary_pos_emb_0,
            rotary_pos_cos_0,
            rotary_pos_sin_0,
            mlp_packed_seq_params_0,
            sequence_len_offset_0,
        )
        torch.cuda.nvtx.range_pop()
        # pre-communicate,0
        signal_0 = self._pre_mlp_to_attn(
            query_0, key_0, value_0, packed_seq_params_0
        )
        signal_0, hidden_states_1 = TickSync.apply(
            compute_stream, comm_stream, signal_0, hidden_states_1
        )

        # 3. pre-attention forward of microbatch 1, mlp2attn all2all of microbatch 0
        # NOTE: do not remove this debug tensor. see above.
        debug_tensors = (query_0, key_0, value_0)
        ## communicate,0
        signal_0 = self._all_to_all(
            signal_0, packed_seq_params_0, is_qkv=True,
        )
        ## compute,1
        torch.cuda.nvtx.range_push("pre_core_attn.1")
        query_1, key_1, value_1, residual_1, attn_mask_type_1 = self._forward_pre_core_attn(
            hidden_states_1,
            rotary_pos_emb_1,
            rotary_pos_cos_1,
            rotary_pos_sin_1,
            mlp_packed_seq_params_1,
            sequence_len_offset_1,
        )
        torch.cuda.nvtx.range_pop()
        # pre-communicate,1
        signal_1 = self._pre_mlp_to_attn(
            query_1, key_1, value_1, packed_seq_params_1
        )
        signal_0, signal_1 = TickSync.apply(
            compute_stream, comm_stream, signal_0, signal_1
        )
        # NOTE: do not remove this debug tensor. see above.
        debug_tensors = (query_1, key_1, value_1)

        # 4. self-attention forward of microbatch 0, mlp2attn all2all of microbatch 1
        ## communicate,1
        signal_1 = self._all_to_all(
            signal_1, packed_seq_params_1, is_qkv=True,
        )
        ## compute
        # post-communicate,0
        query_0, key_0, value_0 = self._post_mlp_to_attn(
            signal_0, packed_seq_params_0
        )
        torch.cuda.nvtx.range_push("core_attn.0")
        core_attn_out_0 = self._forward_core_attn(
            query_0,
            key_0,
            value_0,
            attention_mask_0,
            attention_bias_0,
            attn_mask_type_0,
            packed_seq_params_0,
        )
        torch.cuda.nvtx.range_pop()
        # pre-communicate,0
        signal_0 = self._pre_attn_to_mlp(core_attn_out_0, packed_seq_params_0)
        signal_1, signal_0 = TickSync.apply(
            compute_stream, comm_stream, signal_1, signal_0
        )
        # NOTE: do not remove this debug tensor. see above.
        debug_tensors = core_attn_out_0

        # 5. post-self-attention forward of microbatch 0, mlp2attn all2all of microbatch 1
        ## communicate,0
        signal_0 = self._all_to_all(
            signal_0, packed_seq_params_0, is_qkv=False,
        )
        ## compute,1
        # post-communicate,1
        query_1, key_1, value_1 = self._post_mlp_to_attn(signal_1, packed_seq_params_1)
        torch.cuda.nvtx.range_push("core_attn.1")
        core_attn_out_1 = self._forward_core_attn(
            query_1,
            key_1,
            value_1,
            attention_mask_1,
            attention_bias_1,
            attn_mask_type_1,
            packed_seq_params_1,
        )
        torch.cuda.nvtx.range_pop()
        # pre-communicate,1
        signal_1 = self._pre_attn_to_mlp(core_attn_out_1, packed_seq_params_1)
        signal_0, signal_1 = TickSync.apply(compute_stream, comm_stream, signal_0, signal_1)
        # NOTE: do not remove this debug tensor. see above.
        debug_tensors = core_attn_out_1

        # 6. mlp forward of microbatch 0, mlp2attn all2all of microbatch 1
        # communicate,1
        signal_1 = self._all_to_all(signal_1, packed_seq_params_1, is_qkv=False,)
        ## compute,0
        # post-communicate,0
        core_attn_out_0 = self._post_attn_to_mlp(signal_0, packed_seq_params_0)
        torch.cuda.nvtx.range_push("post_core_attn.0")
        mlp_output_0, context_0 = self._forward_post_core_attn(
            core_attn_out_0,
            residual_0,
            context_0,
            context_mask_0,
        )
        torch.cuda.nvtx.range_pop()
        # no pre-communicate for 0 now.
        signal_1, mlp_output_0 = TickSync.apply(compute_stream, comm_stream, signal_1, mlp_output_0)
        # NOTE: do not remove this debug tensor. see above.
        debug_tensors = None

        torch.cuda.nvtx.range_push("post_core_attn.1")
        # post-communicate,1
        core_attn_out_1 = self._post_attn_to_mlp(signal_1, packed_seq_params_1)
        mlp_output_1, context_1 = self._forward_post_core_attn(
            core_attn_out_1,
            residual_1,
            context_1,
            context_mask_1,
        )
        torch.cuda.nvtx.range_pop()
        # concatenate the two microbatches to one.
        if needs_gather:
            output = _gather_tensor([mlp_output_0, mlp_output_1], num_splits=2)
            context = _gather_tensor([context_0, context_1], num_splits=2)
        else:
            output = [mlp_output_0, mlp_output_1]
            context = [context_0, context_1]
        return output, context

    #### Debug use.
    def forward_ping_pong_single_sided(
        self,
        hidden_states: Tensor,
        attention_mask: Optional[Tensor] = None,
        context: Optional[Tensor] = None,
        context_mask: Optional[Tensor] = None,
        rotary_pos_emb: Optional[Tensor] = None,
        rotary_pos_cos: Optional[Tensor] = None,
        rotary_pos_sin: Optional[Tensor] = None,
        attention_bias: Optional[Tensor] = None,
        inference_context: Optional[Any] = None,
        packed_seq_params: Optional[PingPangSingleStepPackedSeqParams] = None,
        sequence_len_offset: Optional[Tensor] = None,
        *,
        inference_params: Optional[Any] = None,
        return_debug: bool = False,
    ):
        """Debug use. Single side of Ping-Pang parallel."""
        assert inference_params is None, "inference not supported yet"
        assert inference_context is None, "inference not supported yet"
        assert context is None, "cross-attention not supported yet"
        assert context_mask is None, "cross-attention not supported yet"

        setattr(packed_seq_params, "stream", torch.cuda.current_stream())
        backward_resend_qkv = packed_seq_params.bwd_packed_seq_params is not None

        if rotary_pos_emb is not None:
            rotary_pos_emb = None
            warnings.warn("forward_ping_pong_single_sided is only to debug a single ping-pong "
                          "stage's correctness, and does not have RoPE supported")

        query, key, value, residual, attn_mask_type = self._forward_pre_core_attn(
            hidden_states,
            rotary_pos_emb,
            rotary_pos_cos,
            rotary_pos_sin,
            packed_seq_params,
            sequence_len_offset,
        )
        debug_tensors = [(query, key, value),]

        signal = self._pre_mlp_to_attn(query, key, value, packed_seq_params)
        signal = self._all_to_all(signal, packed_seq_params, is_qkv=True)

        if backward_resend_qkv:
            signal = FusedCommAttn.apply(
                signal,
                packed_seq_params.qkv_fwd_metadata,
                packed_seq_params.qkv_bwd_metadata,
                packed_seq_params.attn_out_fwd_metadata,
                packed_seq_params.attn_out_bwd_metadata,
                packed_seq_params,
                packed_seq_params.bwd_packed_seq_params,
                packed_seq_params.dispatcher_id,
                FlashAttnArgs(
                    num_heads_q=self.config.num_attention_heads // self.config.tensor_model_parallel_size,
                    num_heads_kv=self.config.num_query_groups // self.config.tensor_model_parallel_size,
                    head_dim=self.config.hidden_size // self.config.num_attention_heads,
                    return_attn_probs=True,
                    deterministic=True,
                ),
            )
        else:
            query, key, value = self._post_mlp_to_attn(signal, packed_seq_params)
            debug_tensors.append((query, key, value))

            core_attn_out = self._forward_core_attn(
                query,
                key,
                value,
                attention_mask,
                attention_bias,
                attn_mask_type,
                packed_seq_params,
            )
            debug_tensors.append(core_attn_out)

            signal = self._pre_attn_to_mlp(core_attn_out, packed_seq_params)

        signal = self._all_to_all(signal, packed_seq_params, is_qkv=False)

        if backward_resend_qkv:
            core_attn_out = post_a2a_attn_out_with_lse.apply(
                signal, query, key, value,
                self.config.num_attention_heads // self.config.tensor_model_parallel_size,
                packed_seq_params.attn_out_fwd_metadata,
                packed_seq_params.attn_out_bwd_metadata,
                packed_seq_params.dispatcher_id,
            )
        else:
            core_attn_out = self._post_attn_to_mlp(signal, packed_seq_params)
        debug_tensors.append(core_attn_out)

        mlp_output, context = self._forward_post_core_attn(
            core_attn_out,
            residual,
            context,
            context_mask,
        )

        return (mlp_output, context,) + (
            (debug_tensors,) if return_debug else ()
        )


def add_ping_pang_forward(block: MegatronTransformerBlock):
    def init_ping_pang_communication_ctx(self, device: torch.device):
        assert not self.ping_pong_comm_initialized
        self.comm_stream = torch.cuda.Stream(device=device, priority=-1)
        self._ping_pang_debug = True
        self.ping_pong_comm_initialized = True

    def ping_pang_forward(
        self: MegatronTransformerBlock,
        hidden_states: Union[Tensor, WrappedTensor],
        attention_mask: Optional[Tensor],
        context: Optional[Tensor] = None,
        context_mask: Optional[Tensor] = None,
        rotary_pos_emb: Optional[Tensor] = None,
        rotary_pos_cos: Optional[Tensor] = None,
        rotary_pos_sin: Optional[Tensor] = None,
        attention_bias: Optional[Tensor] = None,
        inference_context: Optional[BaseInferenceContext] = None,
        packed_seq_params: Optional[PingPangPackedSeqParams] = None,
        sequence_len_offset: Optional[Tensor] = None,
        *,
        inference_params: Optional[BaseInferenceContext] = None,
    ):
        assert inference_context is None
        assert inference_params is None
        assert context is None
        assert context_mask is None
        assert sequence_len_offset is None
        if self.config.fp8:
            raise NotImplementedError("FP8 not supported yet")

        if isinstance(hidden_states, WrappedTensor):
            hidden_states = hidden_states.unwrap()

        if not self.pre_process:
            # See set_input_tensor()
            hidden_states = self.input_tensor

        hidden_states = make_viewless_tensor(inp=hidden_states, requires_grad=True, keep_graph=True)
        rotary_pos_emb = None

        if self.config.sequence_parallel:
            rng_context = tensor_parallel.get_cuda_rng_tracker().fork()
        else:
            rng_context = nullcontext()
        outer_fp8_context = nullcontext()

        compute_stream = torch.cuda.current_stream()

        packed_seq_params_0 = packed_seq_params.seq_params[0]
        packed_seq_params_1 = packed_seq_params.seq_params[1]
        setattr(packed_seq_params_0, "stream", self.comm_stream)
        setattr(packed_seq_params_1, "stream", self.comm_stream)
        setattr(packed_seq_params_0, "dispatcher_id", 0)
        setattr(packed_seq_params_1, "dispatcher_id", 1)

        with rng_context, outer_fp8_context:
            # Forward pass.
            if self.config.recompute_granularity == 'full' and self.training:
                raise NotImplementedError("Full recompute full layer not supported in ping-pong forward yet.")
            else:
                arg_group = {
                    "hidden_states": hidden_states,
                    "attention_mask": attention_mask,
                    "context": context,
                    "context_mask": context_mask,
                    "rotary_pos_emb": rotary_pos_emb,
                    "rotary_pos_cos": rotary_pos_cos,
                    "rotary_pos_sin": rotary_pos_sin,
                    "attention_bias": attention_bias,
                    "sequence_len_offset": sequence_len_offset,
                }
                arg_group_0, arg_group_1 = _split_all_dict(arg_group, 2)
                del arg_group

                arg_group_0["packed_seq_params"] = packed_seq_params_0
                arg_group_1["packed_seq_params"] = packed_seq_params_1
                arg_group_0["mlp_packed_seq_params"] = packed_seq_params.mlp_layout_seq_params[0]
                arg_group_1["mlp_packed_seq_params"] = packed_seq_params.mlp_layout_seq_params[1]

                for l_no in range(len(self.layers)):
                    inner_fp8_context = nullcontext()
                    with self.offload_context, inner_fp8_context:
                        arg_group_0, arg_group_1, hidden_states, context = self.forward_layers(
                            l_no, arg_group_0, arg_group_1, compute_stream
                        )

                    if (
                        torch.is_grad_enabled()
                        and self.config.cpu_offloading
                        and self.group_prefetch_offload_commit_async is not None
                    ):
                        hidden_states = self.group_prefetch_offload_commit_async(hidden_states)

        # Final layer norm.
        if self.final_layernorm is not None:
            hidden_states = self.final_layernorm(hidden_states)
            # TENorm produces a "viewed" tensor. This will result in schedule.py's
            # deallocate_output_tensor() throwing an error, so a viewless tensor is
            # created to prevent this.
            hidden_states = make_viewless_tensor(
                inp=hidden_states, requires_grad=True, keep_graph=True
            )

        return hidden_states

    # TODO: rename forward_layers -> forward_layer_ping_pong
    def forward_layers(
        self,
        l_no: int,
        arg_group_0: Dict[str, Any],
        arg_group_1: Dict[str, Any],
        compute_stream: torch.cuda.Stream,
    ):
        nvtx_range_push(f"PingPong.forward_layers[{l_no}]")

        layer = self.layers[l_no]
        prev_layer = self.layers[l_no - 1] if l_no > 0 else None

        # tick 0, second half
        with torch.cuda.nvtx.range(f"forward_layers[{l_no}].pre_core_attn.0"):
            arg_group_0 = _forward_pre_core_attn(layer, arg_group_0)
        with torch.cuda.nvtx.range(f"forward_layers[{l_no}].sync_tick.0"):
            _tick_sync(
                compute_stream, self.comm_stream,
                arg_group_0, "signal",  # compute out
                # prev layer's comm out, or anything if it's the first layer
                arg_group_1, "signal" if l_no > 0 else "hidden_states",
            )

        # tick 1
        # communication
        with torch.cuda.nvtx.range(f"forward_layers[{l_no}].all2all_mlp_to_attn.0"):
            arg_group_0 = _layout_mlp_to_attn(layer, arg_group_0)
        # compute
        if l_no > 0:
            with torch.cuda.nvtx.range(f"forward_layers[{l_no}].post_core_attn.1"):
                arg_group_1 = _forward_post_core_attn(prev_layer, arg_group_1)
        with torch.cuda.nvtx.range(f"forward_layers[{l_no}].pre_core_attn.1"):
            arg_group_1 = _forward_pre_core_attn(layer, arg_group_1)
        with torch.cuda.nvtx.range(f"forward_layers[{l_no}].sync_tick.1"):
            _tick_sync(
                compute_stream, self.comm_stream,
                arg_group_0, "signal",  # comm out
                arg_group_1, "signal",  # compute out
            )

        # tick 2
        # communication
        with torch.cuda.nvtx.range(f"forward_layers[{l_no}].all2all_mlp_to_attn.1"):
            arg_group_1 = _layout_mlp_to_attn(layer, arg_group_1)
        # compute
        with torch.cuda.nvtx.range(f"forward_layers[{l_no}].core_attn.0"):
            arg_group_0 = _forward_core_attn(layer, arg_group_0)
        with torch.cuda.nvtx.range(f"forward_layers[{l_no}].sync_tick.2"):
            _tick_sync(
                compute_stream, self.comm_stream,
                arg_group_0, "signal",  # compute out
                arg_group_1, "signal",  # comm out.
            )

        # tick 3
        # communication
        with torch.cuda.nvtx.range(f"forward_layers[{l_no}].all2all_attn_to_mlp.0"):
            arg_group_0 = _layout_attn_to_mlp(layer, arg_group_0)
        # compute
        with torch.cuda.nvtx.range(f"forward_layers[{l_no}].core_attn_1"):
            arg_group_1 = _forward_core_attn(layer, arg_group_1)
        with torch.cuda.nvtx.range(f"forward_layers[{l_no}].sync_tick.3"):
            _tick_sync(
                compute_stream, self.comm_stream,
                arg_group_0, "signal",  # comm out
                arg_group_1, "signal",  # compute out
            )

        # tick 4, also the tick 0 of the next layer
        # communication
        with torch.cuda.nvtx.range(f"forward_layers[{l_no}].all2all_attn_to_mlp.1"):
            arg_group_1 = _layout_attn_to_mlp(layer, arg_group_1)
        # compute
        with torch.cuda.nvtx.range(f"forward_layers[{l_no}].post_core_attn.0"):
            arg_group_0 = _forward_post_core_attn(layer, arg_group_0)
        # NOTE: sync of this tick is at the next layer.

        # if the last layer, do the other half of tick 4 and tick 5
        if l_no == len(self.layers) - 1:
            # No next layer, do the sync here.
            with torch.cuda.nvtx.range(f"forward_layers[{l_no}].sync_tick.4"):
                _tick_sync(
                    compute_stream, self.comm_stream,
                    arg_group_0, "hidden_states",   # place holder
                    arg_group_1, "signal",          # comm out
                )
            with torch.cuda.nvtx.range(f"forward_layers[{l_no}].post_core_attn.1"):
                arg_group_1 = _forward_post_core_attn(layer, arg_group_1)
            # gathering the result
            with torch.cuda.nvtx.range(f"forward_layers[{l_no}].gather_ping_pong"):
                hidden_states = _gather_tensor([arg_group_0["hidden_states"], arg_group_1["hidden_states"]], 2)
                context = _gather_tensor([arg_group_0["context"],arg_group_1["context"]], 2)
        else:
            hidden_states = None
            context = None

        nvtx_range_pop(f"PingPong.forward_layers[{l_no}]")

        return arg_group_0, arg_group_1, hidden_states, context

    def _forward_pre_core_attn(layer: TransformerLayer, args: Dict[str, Any]):
        hidden_states = args.pop("hidden_states")
        query, key, value, residual, attn_mask_type = layer._forward_pre_core_attn(
            hidden_states,
            args["rotary_pos_emb"],
            args["rotary_pos_cos"],
            args["rotary_pos_sin"],
            args["mlp_packed_seq_params"],
            args["sequence_len_offset"],
        )
        signal = layer._pre_mlp_to_attn(query, key, value, args["packed_seq_params"])
        args["query"] = query
        args["key"] = key
        args["value"] = value
        args["residual"] = residual
        args["attn_mask_type"] = attn_mask_type
        args["signal"] = signal
        return args

    def _layout_mlp_to_attn(layer: TransformerLayer, args: Dict[str, Any]):
        bwd_resend_qkv = args["packed_seq_params"].bwd_packed_seq_params is not None
        if not bwd_resend_qkv:
            # qkv are stored until attn_out to resend at backward.
            args.pop("query"), args.pop("key"), args.pop("value")
        signal = args.pop("signal")
        args["signal"] = layer._all_to_all(signal, args["packed_seq_params"], is_qkv=True)
        return args

    def _forward_core_attn(layer: TransformerLayer, args: Dict[str, Any]):
        # pop out to make sure the tensor is freed
        signal = args.pop("signal")
        packed_seq_params: PingPangSingleStepPackedSeqParams = args["packed_seq_params"]
        bwd_resend_qkv = packed_seq_params.bwd_packed_seq_params is not None
        if bwd_resend_qkv:
            signal = FusedCommAttn.apply(
                signal,
                packed_seq_params.qkv_fwd_metadata,
                packed_seq_params.qkv_bwd_metadata,
                packed_seq_params.attn_out_fwd_metadata,
                packed_seq_params.attn_out_bwd_metadata,
                packed_seq_params,
                packed_seq_params.bwd_packed_seq_params,
                packed_seq_params.dispatcher_id,
                FlashAttnArgs(
                    num_heads_q=layer.config.num_attention_heads // layer.config.tensor_model_parallel_size,
                    num_heads_kv=layer.config.num_query_groups // layer.config.tensor_model_parallel_size,
                    head_dim=layer.config.hidden_size // layer.config.num_attention_heads,
                    return_attn_probs=True,
                    deterministic=True,
                ),
            )
            args["signal"] = signal
        else:
            query, key, value = layer._post_mlp_to_attn(signal, args["packed_seq_params"])
            core_attn_out = layer._forward_core_attn(
                query, key, value,
                args["attention_mask"],
                args["attention_bias"],
                args["attn_mask_type"],
                args["packed_seq_params"],
            )
            args["signal"] = layer._pre_attn_to_mlp(core_attn_out, args["packed_seq_params"])
        return args

    def _layout_attn_to_mlp(layer: TransformerLayer, args: Dict[str, Any]):
        signal = args.pop("signal")
        args["signal"] = layer._all_to_all(signal, args["packed_seq_params"], is_qkv=False)
        return args

    def _forward_post_core_attn(layer: TransformerLayer, args: Dict[str, Any]):
        signal = args.pop("signal")
        packed_seq_params: PingPangSingleStepPackedSeqParams = args["packed_seq_params"]
        bwd_resend_qkv = packed_seq_params.bwd_packed_seq_params is not None
        if bwd_resend_qkv:
            core_attn_out = post_a2a_attn_out_with_lse.apply(
                signal, args["query"], args["key"], args["value"],
                layer.config.num_attention_heads // layer.config.tensor_model_parallel_size,
                packed_seq_params.attn_out_fwd_metadata,
                packed_seq_params.attn_out_bwd_metadata,
                packed_seq_params.dispatcher_id,
            )
            args.pop("query"), args.pop("key"), args.pop("value")
        else:
            core_attn_out = layer._post_attn_to_mlp(signal, args["packed_seq_params"])
        residual = args.pop("residual")
        mlp_output, context = layer._forward_post_core_attn(
            core_attn_out,
            residual,
            args["context"],
            args["context_mask"],
        )
        args["hidden_states"] = mlp_output
        args["context"] = context
        return args

    def _tick_sync(compute_stream, comm_stream, arg_group_0, keys_0, arg_group_1, keys_1):
        if isinstance(keys_0, str):
            keys_0 = [keys_0]
        if isinstance(keys_1, str):
            keys_1 = [keys_1]
        tensors_0 = [arg_group_0[key] for key in keys_0]
        tensors_1 = [arg_group_1[key] for key in keys_1]
        tensors = tensors_0 + tensors_1
        out_tensors = TickSync.apply(compute_stream, comm_stream, *tensors)
        out_tensors_0 = out_tensors[:len(tensors_0)]
        out_tensors_1 = out_tensors[len(tensors_0):]
        for key, out_tensor in zip(keys_0, out_tensors_0):
            arg_group_0[key] = out_tensor
        for key, out_tensor in zip(keys_1, out_tensors_1):
            arg_group_1[key] = out_tensor

    class _debug_monkey_patch:
        def __init__(self, layer_forward_impl):
            self._layer_forward_impl = layer_forward_impl
        def __enter__(self):
            self.backup_forward = TransformerLayer.forward
            TransformerLayer.forward = self._layer_forward_impl
        def __exit__(self, exc_type, exc_value, traceback):
            TransformerLayer.forward = self.backup_forward

    def forward(self, *args, **kwargs):
        # print(f'{self._ping_pang_debug=}')
        """
        For Pipeline Parallel debugging, we use single-sided to ease debugging.
        """
        if self._ping_pang_debug:
            assert self._debug_forward_impl in ["orig", "single_sided", "orig_reimpl"], self._debug_forward_impl
            if self._debug_forward_impl == "single_sided":
                ctx = _debug_monkey_patch(TransformerLayer.forward_ping_pong_single_sided)
            elif self._debug_forward_impl == "orig_reimpl":
                ctx = _debug_monkey_patch(TransformerLayer.forward_orig_impl)
            else:
                ctx = nullcontext()
            with ctx:
                return self._normal_forward(*args, **kwargs)

        assert self.ping_pong_comm_initialized
        return self.ping_pang_forward(*args, **kwargs)

    block._debug_forward_impl = "orig"
    block.forward_layers = types.MethodType(forward_layers, block)
    block.init_ping_pang_communication_ctx = types.MethodType(init_ping_pang_communication_ctx, block)
    block.ping_pang_forward = types.MethodType(ping_pang_forward, block)
    block._normal_forward = block.forward
    block.forward = types.MethodType(forward, block)
    block.ping_pong_comm_initialized = False


class PingPangGPTModel(GPTModel):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        add_ping_pang_forward(self.decoder)

    def set_debug(self, debug: bool, debug_fwd_impl: str = None):
        self.decoder._ping_pang_debug = debug
        if debug_fwd_impl:
            self.decoder._debug_forward_impl = debug_fwd_impl

    # very ugly hardcode here:
    # If the forward is a dummy forward, then:
    # 1. ensure to skip calculating the loss
    # 2. ensure to have a dummy hidden_states with correct shape
    #    - this prevents error in decoder.final_layernorm
    #    - this make layer.forward easier
    @contextmanager
    def _reset_stage_for_dummy_forward(self, reset):
        if not reset:
            yield
        else:
            decoder_pre_process = self.decoder.pre_process
            post_process = self.post_process
            self.decoder.pre_process = True
            self.post_process = False
            yield
            self.post_process = post_process
            self.decoder.pre_process = decoder_pre_process

    @functools.wraps(GPTModel.forward)
    def forward(self, input_ids, *args, **kwargs):
        # print(f'{len(self.decoder.layers)=}')
        is_dummy_forward = getattr(self.decoder.layers[0], "current_microbatch", 0) < 0
        if is_dummy_forward and not self.pre_process:
            # get dtype
            dtype = (
                torch.bfloat16 if self.config.bf16
                else torch.float16 if self.config.fp16
                else self.config.params_dtype
            )
            # create a dummy decoder_input
            sl = input_ids.shape[-1]
            hs = self.config.hidden_size
            if self.config.sequence_parallel:
                sl //= self.config.tensor_model_parallel_size
            kwargs['decoder_input'] = torch.zeros((sl, 1, hs), dtype=dtype, device='cuda')

        with self._reset_stage_for_dummy_forward(reset=is_dummy_forward):
            output = super().forward(input_ids, *args, **kwargs)

        return output

    def dummy_backward(self, packed_seq_params: PingPangPackedSeqParams):
        """
        A dummy backward that runs #layer times of decoder layer's backward.
        When the device is idle (at pipeline pre-fill or drain-out period),
        this makes it serve as a remote Core-Attention Server.
        """
        dtype = self.decoder.layers[0].self_attention.linear_qkv.weight.dtype
        device = self.decoder.layers[0].self_attention.linear_qkv.weight.device
        for _ in self.decoder.layers:
            dummy_backward(self.config, packed_seq_params, dtype, device)

    def init_ping_pong_communication_ctx(self, device: torch.device):
        self.decoder.init_ping_pang_communication_ctx(device)
