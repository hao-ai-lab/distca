from typing import Sequence

import torch
from torch import Tensor

from d2.runtime.inplace_metadata import Metadata
from d2.runtime.fast_alltoall_metadata import (
    compute_forward_qkv_a2a_layout_meatadata,
    compute_backward_qkv_a2a_layout_metadata,
    FastAlltoAllMetadata
)

from test_util import create_qkv_dispatch


def simulate_fa2a_send_copy_non_cp(
    q: Tensor, dst_tensor: Tensor, q_offsets: Tensor, q_seqlen: Tensor,
    hidden_size_q: int, element_size: int
):

    q_seq_offset = 0
    for seq_id, seq_len in enumerate(q_seqlen):
        q_offset_bytes = q_offsets[seq_id]
        q_dst_offset = q_offset_bytes // element_size
        q_size = seq_len * hidden_size_q
        dst_tensor[q_dst_offset:q_dst_offset + q_size] = (
            q[q_seq_offset:q_seq_offset + q_size]
        )
        q_seq_offset += q_size
    return dst_tensor


def simulate_fa2a_send_copy_cp(
    k: Tensor, dst_tensor: Tensor,
    k_offset: Tensor, k_seqlen: Tensor,
    hidden_size_k: int, element_size: int, max_cp: int,
    send_dst: Tensor
):
    """
    Simulate the fa2a send: MLP layout (CP) -> a2a layout.
    MLP layout (CP) is in form of: (seq_len,), but has multiple copies
    to the a2a layout.
    """
    k_offset = 0
    for cp_id in range(max_cp):
        k_seq_offset = 0
        for seq_id, seq_len in enumerate(k_seqlen):
            k_size = seq_len * hidden_size_k
            if send_dst[cp_id, seq_id] >= 0:
                k_seq_offset += k_size
                continue
            k_offset_bytes = k_offset[cp_id, seq_id]
            k_dst_offset = k_offset_bytes // element_size
            dst_tensor[k_dst_offset:k_dst_offset + k_size] = (
                k[k_seq_offset:k_seq_offset + k_size]
            )
            k_seq_offset += k_size
    return dst_tensor


def simulate_fa2a_recv_copy_non_cp(
    q: Tensor, src_tensor: Tensor, q_offsets: Tensor, q_seqlen: Tensor,
    hidden_size_q: int, element_size: int
):
    """Simulate the fa2a recv: a2a layout -> non-CP layout."""
    q_seq_offset = 0
    for seq_id, seq_len in enumerate(q_seqlen):
        q_offset_bytes = q_offsets[seq_id]
        q_dst_offset = q_offset_bytes // element_size
        q_size = seq_len * hidden_size_q
        q[q_seq_offset:q_seq_offset + q_size] = (
            src_tensor[q_dst_offset:q_dst_offset + q_size]
        )
        q_seq_offset += q_size
    return q


def simulate_fa2a_recv_copy_kv_grad(
    k: Tensor, src_tensor: Tensor,
    k_offset: Tensor, k_seqlen: Tensor,
    hidden_size_k: int, element_size: int, max_cp: int,
    send_dst: Tensor
):
    """
    Simulate the fa2a recv: a2a layout -> MLP grad CP layout.
    MLP grad CP layout is in form of: (max_cp, seq_len)
    """
    k_offset = 0
    for cp_id in range(max_cp):
        k_seq_offset = 0
        for seq_id, seq_len in enumerate(k_seqlen):
            k_size = seq_len * hidden_size_k
            if send_dst[cp_id, seq_id] >= 0:
                k_seq_offset += k_size
                continue
            k_offset_bytes = k_offset[cp_id, seq_id]
            k_dst_offset = k_offset_bytes // element_size
            k[cp_id, k_seq_offset:k_seq_offset + k_size] = (
                src_tensor[k_dst_offset:k_dst_offset + k_size]
            )
            k_seq_offset += k_size
    return k


def simulate_fa2a_send_qkv_copy(
    q: Tensor, k: Tensor, v: Tensor,
    dst_tensor: Tensor, q_offset: Tensor, k_offset: Tensor, v_offset: Tensor,
    q_seqlen: Tensor, k_seqlen: Tensor,
    hidden_size_q: int, hidden_size_k: int, element_size: int, max_cp: int,
    kv_send_dst: Tensor,
):
    q = q.view(-1)
    k = k.view(-1)
    v = v.view(-1)
    dst_tensor = simulate_fa2a_send_copy_non_cp(
        q, dst_tensor, q_offset, q_seqlen, hidden_size_q, element_size
    )
    dst_tensor = simulate_fa2a_send_copy_cp(
        k, dst_tensor, k_offset, k_seqlen, hidden_size_k, element_size,
        max_cp, kv_send_dst
    )
    dst_tensor = simulate_fa2a_send_copy_cp(
        v, dst_tensor, v_offset, k_seqlen, hidden_size_k, element_size,
        max_cp, kv_send_dst
    )
    return dst_tensor


def simulate_fa2a_recv_copy_qkv(
    q: Tensor, k: Tensor, v: Tensor,
    src_tensor: Tensor, q_offset: Tensor, k_offset: Tensor, v_offset: Tensor,
    q_seqlen: Tensor, k_seqlen: Tensor,
    hidden_size_q: int, hidden_size_k: int, element_size: int
):
    q = q.view(-1)
    k = k.view(-1)
    v = v.view(-1)
    q = simulate_fa2a_recv_copy_non_cp(
        q, src_tensor, q_offset, q_seqlen, hidden_size_q, element_size
    )
    k = simulate_fa2a_recv_copy_non_cp(
        k, src_tensor, k_offset, k_seqlen, hidden_size_k, element_size
    )
    v = simulate_fa2a_recv_copy_non_cp(
        v, src_tensor, v_offset, k_seqlen, hidden_size_k, element_size
    )
    return q, k, v


def simulate_fa2a(send_buffer: Tensor, recv_buffer: Tensor,
                  fa2a_metadata: Sequence[Tensor], element_size: int):
    world_size = send_buffer.shape[0]
    assert recv_buffer.shape[0] == world_size
    (sender_send_disp, sender_transfer_sz, sender_recv_disp,
     recver_transfer_sz) = fa2a_metadata
    for src_rank in range(world_size):
        for dst_rank in range(world_size):
            send_offset = sender_send_disp[src_rank, dst_rank] // element_size
            recv_offset = sender_recv_disp[src_rank, dst_rank] // element_size
            size = sender_transfer_sz[src_rank, dst_rank] // element_size
            recv_buffer[dst_rank][recv_offset:recv_offset + size] = (
                send_buffer[src_rank][send_offset:send_offset + size]
            )
    return recv_buffer


def simulate_qkv_a2a_fwd(
    q: Tensor, k: Tensor, v: Tensor,
    metadata: FastAlltoAllMetadata,
    q_metadata: Metadata, kv_metadata: Metadata,
    element_size: int, hidden_q: int, hidden_k: int
):
    world_size = q.shape[0]
    assert k.shape[0] == world_size
    assert v.shape[0] == world_size
    # sender transfer sz
    tot_send_bytes = metadata.fa2a_metadata[1].sum(dim=1) // element_size
    max_send_bytes = int(torch.max(tot_send_bytes).item())
    tot_recv_bytes = metadata.fa2a_metadata[3].sum(dim=1) // element_size
    max_recv_bytes = int(torch.max(tot_recv_bytes).item())

    src_buffer = torch.zeros(
        (world_size, max_send_bytes), dtype=q.dtype, device=q.device
    )
    dst_buffer = torch.zeros(
        (world_size, max_recv_bytes), dtype=q.dtype, device=q.device
    )
    for rank in range(world_size):
        metadata_slice = metadata.get_slice(rank)
        q_metadata_slice = q_metadata.get_slice(rank)
        kv_metadata_slice = kv_metadata.get_slice(rank)
        q_src_offsets, k_src_offsets, v_src_offsets = metadata_slice.send_memcpy_metadata
        q_src_seqlen = q_metadata_slice.seq_len
        kv_src_seqlen = kv_metadata_slice.seq_len
        max_cp = kv_metadata_slice.dst_rank.shape[1]

        src_buffer[rank] = simulate_fa2a_send_qkv_copy(
            q[rank], k[rank], v[rank], src_buffer[rank],
            q_src_offsets, k_src_offsets, v_src_offsets,
            q_src_seqlen, kv_src_seqlen,
            hidden_q, hidden_k, element_size, max_cp,
            kv_metadata_slice.dst_rank
        )

    dst_buffer = simulate_fa2a(
        src_buffer, dst_buffer, metadata.fa2a_metadata, element_size
    )

    num_recv_tokens_q = q_metadata.num_recv_tokens[:, -1]
    max_num_recv_tokens_q = int(torch.max(num_recv_tokens_q).item())
    num_recv_tokens_k = kv_metadata.num_recv_tokens[:, -1]
    max_num_recv_tokens_k = int(torch.max(num_recv_tokens_k).item())
    dst_q = torch.zeros(
        (world_size, max_num_recv_tokens_q * hidden_q),
        device=q.device, dtype=q.dtype
    )
    dst_k = torch.zeros(
        (world_size, max_num_recv_tokens_k * hidden_k),
        device=k.device, dtype=k.dtype
    )
    dst_v = dst_k.clone()
    for rank in range(world_size):
        metadata_slice = metadata.get_slice(rank)
        q_metadata_slice = q_metadata.get_slice(rank)
        kv_metadata_slice = kv_metadata.get_slice(rank)
        q_recv_offsets, k_recv_offsets, v_recv_offsets = metadata_slice.recv_memcpy_metadata
        max_cp = kv_metadata_slice.dst_rank.shape[1]

        q_slice, k_slice, v_slice = simulate_fa2a_recv_copy_qkv(
            dst_q[rank], dst_k[rank], dst_v[rank],
            dst_buffer[rank], q_recv_offsets, k_recv_offsets, v_recv_offsets,
            recv_seq_lens_q, recv_seq_lens_kv,
            hidden_q, hidden_k, element_size
        )
        dst_q[rank] = q_slice
        dst_k[rank] = k_slice
        dst_v[rank] = v_slice
