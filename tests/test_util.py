from dataclasses import dataclass
import logging
import math
import os
import random
import socket
from typing import Optional
from datetime import timedelta

from megatron.core import parallel_state as mpu
import ray
import torch
import time

from d2.runtime.attn_kernels.ops import (
    nvshmem_get_unique_id, nvshmem_alloc_empty_unique_id, DispatcherWrapper
)
from d2.runtime.compute_metadata import (
    from_planner_output, backward_from_planner_output,
)
from d2.runtime.shard_info import ShardInfo
from d2.runtime.megatron.create_group import (
    initialize_attention_server_comm, get_attn_server_group_gloo,
    get_attn_server_rank, get_attn_server_group_src_rank
)
from d2.planner.planner import batch_to_items_with_dummy, cp_list_to_mlp_list


logger = logging.getLogger(__name__)


######## MISC
def get_device_name() -> str:
    return "cuda"


def get_torch_device() -> any:
    """Return the corresponding torch attribute based on the device type string.
    Returns:
        module: The corresponding torch device namespace, or torch.cuda if not found.
    """
    device_name = get_device_name()
    try:
        return getattr(torch, device_name)
    except AttributeError:
        logger.warning(f"Device namespace '{device_name}' not found in torch, try to load torch.cuda.")
        return torch.cuda


def set_random_seed(seed, set_megatron: bool=True):
    """Set worker side random seed."""
    import random

    import numpy as np
    import torch

    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if get_torch_device().device_count() > 0 and set_megatron:
        from megatron.core import tensor_parallel

        tensor_parallel.model_parallel_cuda_manual_seed(seed)


######## Workers
@dataclass
class ParallelConfig:
    tensor_model_parallel_size: int = 1
    pipeline_model_parallel_size: int = 1
    virtual_pipeline_model_parallel_size: Optional[int] = None
    context_parallel_size: int = 1
    expert_model_parallel_size: int = 1
    expert_tensor_parallel_size: Optional[int] = None


# NOTE: the Worker abstraction is to make it compatible with ray.
# However, since ray has some issue with nsys, our default launch is torchrun.
class BaseWorker:
    def __init__(self, rank: int, world_size: int):
        self.rank = rank
        self.world_size = world_size
        self.device = None

    def init_torch_distributed(self,):
        if not torch.distributed.is_initialized():

            timeout = None
            EXPERIMENT_TORCH_DIST_TIMEOUT = os.environ.get("EXPERIMENT_TORCH_DIST_TIMEOUT", "-1")
            try:
                EXPERIMENT_TORCH_DIST_TIMEOUT = int(EXPERIMENT_TORCH_DIST_TIMEOUT)
                if EXPERIMENT_TORCH_DIST_TIMEOUT > 0:
                    timeout = timedelta(seconds=EXPERIMENT_TORCH_DIST_TIMEOUT)
            except:
                pass
                
            torch.distributed.init_process_group(
                backend="cpu:gloo,cuda:nccl", rank=self.rank, world_size=self.world_size, timeout=timeout
            )
            try:
                local_rank = int(os.environ.get("LOCAL_RANK"))
                self.device = torch.device(f"cuda:{local_rank}")
                torch.cuda.set_device(self.device)
            except:
                # In slurm environment, only have one visible GPU per node.
                torch.cuda.set_device('cuda')
                pass

    def init_nvshmem(self, buffer_size: int, local_rank: int = None):
        if self.rank == 0:
            uid = nvshmem_get_unique_id()
        else:
            uid = nvshmem_alloc_empty_unique_id()
        torch.distributed.broadcast(uid, src=0)

        DispatcherWrapper.init(
            self.rank, local_rank, self.world_size, buffer_size, uid
        )

    def init_comm(self, buffer_size: int, local_rank: int = None):
        if local_rank is None:
            local_rank = int(os.getenv("LOCAL_RANK"))

        self.init_torch_distributed()
        self.init_nvshmem(buffer_size, local_rank)

    #### General init functions for ray.
    def get_node_ip_port(self):
        host_ipv4 = os.getenv("MY_HOST_IP", None)
        host_ipv6 = os.getenv("MY_HOST_IPV6", None)
        host_ip_by_env = host_ipv4 or host_ipv6
        host_ip_by_sdk = ray._private.services.get_node_ip_address()

        host_ip = host_ip_by_env or host_ip_by_sdk

        with socket.socket() as sock:
            sock.bind(("", 0))
            port = sock.getsockname()[1]
        return host_ip, str(port)

    def set_master_addr_port(self, master_addr: str, master_port: str):
        os.environ["MASTER_ADDR"] = master_addr
        os.environ["MASTER_PORT"] = master_port

    # def shutdown(self):
    #     torch.distributed.destroy_process_group()

    # def __del__(self):
    #     self.shutdown()


class MegatronBaseWorker(BaseWorker):
    """Worker base class to init communication groups (megatron and nvshmem)."""
    def __init__(self, rank: int, world_size: int):
        super().__init__(rank, world_size)
        self.as_world_size = None
        self.as_rank = None

    def init_nvshmem(self, buffer_size: int, parallel_config: ParallelConfig, local_rank: int=None):
        if local_rank is None:
            local_rank = int(os.getenv("LOCAL_RANK"))

        tp_size = parallel_config.tensor_model_parallel_size
        if tp_size == 1:
            super().init_nvshmem(buffer_size, local_rank)
            self.as_world_size = self.world_size
            self.as_rank = self.rank

        initialize_attention_server_comm()
        group = get_attn_server_group_gloo()
        as_world_size = torch.distributed.get_world_size(group=group)
        as_rank = get_attn_server_rank()
        as_src_rank = get_attn_server_group_src_rank()

        self.as_world_size = as_world_size
        self.as_rank = as_rank
        if as_rank == 0:
            uid = nvshmem_get_unique_id()
        else:
            uid = nvshmem_alloc_empty_unique_id()
        # print(f"[Rank {as_rank}] init nvshmem with uid = {uid}")
        torch.distributed.broadcast(uid, src=as_src_rank, group=group)
        # print(f"[Rank {as_rank}] after broadcast uid = {uid}")
        DispatcherWrapper.init(
            as_rank, local_rank, as_world_size, buffer_size, uid
        )


    def init_comm(self, buffer_size: int, parallel_config: ParallelConfig, local_rank: Optional[int] = None):
        # Init megatron communication.
        self.init_torch_distributed()
        # NOTE: do not set to local_rank here because the cuda visible device is set by ray.
        mpu.initialize_model_parallel(
            tensor_model_parallel_size=parallel_config.tensor_model_parallel_size,
            pipeline_model_parallel_size=parallel_config.pipeline_model_parallel_size,
            virtual_pipeline_model_parallel_size=parallel_config.virtual_pipeline_model_parallel_size,
            pipeline_model_parallel_split_rank=None,
            use_sharp=False,
            context_parallel_size=parallel_config.context_parallel_size,
            expert_model_parallel_size=parallel_config.expert_model_parallel_size,
            expert_tensor_parallel_size=parallel_config.expert_tensor_parallel_size,
            nccl_communicator_config_path=None,
        )
        self.init_nvshmem(buffer_size, parallel_config, local_rank)


def init_worker_torch_distributed(
    world_size, buffer_size, worker_cls, parallel_config
):
    assert world_size == int(os.environ.get("WORLD_SIZE")), f"world_size: {world_size} != WORLD_SIZE: {os.environ.get('WORLD_SIZE')}"
    rank = int(os.environ.get("RANK"))
    local_rank = int(os.environ.get("LOCAL_RANK"))
    worker = worker_cls(
        rank, world_size
    )
    if parallel_config is not None:
        worker.init_comm(buffer_size, parallel_config, local_rank)
    else:
        worker.init_comm(buffer_size, local_rank)
    return worker


######## Data construction
def create_list(n: int, s: int, min_val: int, t: int) -> list[int] | None:
    """
    Generates a list of n integers that sum to s.

    Each integer in the list must be:
    - in range of [min_val, s).
    - Divisible by t.

    Returns:
        A list of integers meeting the criteria, or None if no solution exists.
    """
    # --- 1. Input Validation ---
    assert n > 0
    assert s % t == 0

    # --- 2. Determine Valid Range for Numbers ---
    if t > 1:
        min_val = math.ceil((min_val + 1) / t) * t
    assert s >= n * min_val

    # --- 3. Construct the List ---
    # Start with a list where every element is the minimum possible value.
    result = [min_val] * n
    remainder = (s - min_val * n) // t
    remain_result = torch.rand((n,))
    remain_result = (remain_result / remain_result.sum() * remainder).to(torch.int)
    # handle rounding error
    if torch.sum(remain_result).item() != remainder:
        diff = remainder - sum(remain_result)
        if diff > 0:
            remain_result[-1] += diff
        else:
            idx = 0
            while remain_result[idx] <= diff: idx += 1
            remain_result[idx] += diff
            assert remain_result[idx] > 0
    remain_result = (remain_result * t).tolist()
    for rid in range(n):
        result[rid] += remain_result[rid]
    assert sum(result) == s

    return result


def create_random_dispatch_from_existing(
    per_rank_shard_lens: list[list[list[int]]], world_size: int, merge_ranks: bool=True,
):
    assert len(per_rank_shard_lens) == world_size
    num_shards = sum(len(doc_shards) for rank_shards in per_rank_shard_lens for doc_shards in rank_shards)
    dsts = torch.concat([
        torch.range(0, world_size - 1, dtype=torch.int32, device="cpu"),
        torch.randint(0, world_size, (num_shards - world_size,), dtype=torch.int32, device="cpu"),
    ], dim=0)
    dsts = dsts[torch.randperm(num_shards, device="cpu")].tolist()
    dsts_iter = iter(dsts)
    outs = []
    for rank in range(world_size):
        rank_outs = []
        for doc_shards in per_rank_shard_lens[rank]:
            doc_outs = []
            for sid, shard_len in enumerate(doc_shards):
                doc_outs.append(
                    ShardInfo(rid=rank, dispatch_rid=next(dsts_iter),
                              logical_sid=sid, shard_len=shard_len)
                )
            rank_outs.append(doc_outs)
        outs.append(rank_outs)
    if merge_ranks:
        outs = [doc_shards for rank_out in outs for doc_shards in rank_out]
    return outs


def create_random_shard_info(
    seed: int, world_size: int, num_doc: int,
    max_num_shard: int, max_shard_len: int=-1, min_shard_len: int=8,
    tot_num_token: int=-1, multiple_of: int=1
):
    set_random_seed(seed, set_megatron=False)
    scheduler_output: list[list[ShardInfo]] = []
    if max_shard_len <= 0:
        assert tot_num_token > 0
        max_shard_len = tot_num_token

    num_shards = torch.randint(1, max_num_shard + 1, (num_doc,)).tolist()
    has_shard_src = [False] * world_size
    has_shard_dst = [False] * world_size
    src_num_token = [0] * world_size
    for doc_id in range(num_doc):
        num_shard = num_shards[doc_id]
        doc_schedule = []
        for shard_id in range(num_shard):
            rid = random.randint(0, world_size - 1)
            d_rid = random.randint(0, world_size - 1)
            has_shard_src[rid] = True
            has_shard_dst[d_rid] = True
            shard_len = random.randint(min_shard_len, max_shard_len)
            src_num_token[rid] += shard_len
            doc_schedule.append(
                ShardInfo(rid=rid, dispatch_rid=d_rid, logical_sid=shard_id, shard_len=shard_len)
            )
        scheduler_output.append(doc_schedule)
    for rank in range(world_size):
        if not has_shard_src[rank]:
            scheduler_output.append([ShardInfo(rid=rank, dispatch_rid=rank, logical_sid=0, shard_len=min_shard_len)])
            has_shard_src[rank] = True
            has_shard_dst[rank] = True
            src_num_token[rank] += min_shard_len
        if not has_shard_dst[rank]:
            scheduler_output.append([ShardInfo(rid=rank, dispatch_rid=rank, logical_sid=0, shard_len=min_shard_len)])
            has_shard_src[rank] = True
            has_shard_dst[rank] = True
            src_num_token[rank] += min_shard_len

    if tot_num_token > 0:
        shards = [[] for _ in range(world_size)]
        for s in scheduler_output:
            for shard in s:
                shards[shard.rid].append(shard)
        for ss in shards:
            num_shards = len(ss)
            shard_lens = create_list(num_shards, tot_num_token, min_shard_len, multiple_of)
            for shard, l in zip(ss, shard_lens):
                shard.shard_len = l
        src_num_token = [tot_num_token] * world_size

    return scheduler_output, src_num_token


def random_shard_info_linear_layout_dp(
    world_size: int, max_num_doc_per_rank: int, tot_num_token: int,
    min_shard_len: int=8, multiple_of: int=1, max_num_shard: int=4, seed: int = None,
    create_backward_redispatch: bool=False,
):
    """
    Create random shard info but guarantee that documents are stored
    in a DP manner on the linear layout.
    For backward redispatch, ideally we should consider that a forward
    shard may be further split into multiple shards during backward,
    but for simplicity we skip this now.
    """
    if seed is not None:
        set_random_seed(seed, set_megatron=False)
    scheduler_output_per_rank: list[list[list[ShardInfo]]] = [[] for _ in range(world_size)]
    has_shard_dst = [False] * world_size
    glob_doc_lens = [[] for _ in range(world_size)]
    for r in range(world_size):
        num_doc = random.randint(1, max_num_doc_per_rank)
        doc_lens = create_list(num_doc, tot_num_token, min_shard_len, multiple_of)
        glob_doc_lens[r] = doc_lens
        for doc_len in doc_lens:
            max_shard = min(max_num_shard, doc_len // min_shard_len)
            num_shard = random.randint(1, max_shard)
            shard_lens = create_list(num_shard, doc_len, min_shard_len, multiple_of)
            shards = [
                ShardInfo(
                    r, dispatch_rid=random.randint(0, world_size - 1),
                    logical_sid=shard_id, shard_len=shard_len
                ) for shard_id, shard_len in enumerate(shard_lens)
            ]
            for s in shards:
                has_shard_dst[s.dispatch_rid] = True
            scheduler_output_per_rank[r].append(shards)

    # guarantee that each rank is at least dst of one shard.
    scheduler_output: list[list[ShardInfo]] = []
    for rank in range(world_size):
        if not has_shard_dst[rank]:
            # 1. find a doc to modify.
            token_updated = False
            for did in range(glob_doc_lens[rank]):
                for shard in scheduler_output_per_rank[rank][did]:
                    if shard.shard_len < 2 * min_shard_len:
                        continue
                    # found the document, modify
                    glob_doc_lens[rank][did] -= min_shard_len
                    shard.shard_len -= min_shard_len
                    token_updated = True
                    break
                if token_updated:
                    break
            # add a new smallest document.
            scheduler_output_per_rank[rank].append([
                ShardInfo(rid=rank, dispatch_rid=rank, logical_sid=0,
                          shard_len=min_shard_len)
            ])
            glob_doc_lens[rank].append(min_shard_len)
        scheduler_output.extend(scheduler_output_per_rank[rank])

    # if need to create a backward redispatch, create a new dispatch_rid.
    if create_backward_redispatch:
        per_rank_shard_lens = [[
                [s.shard_len for s in ds] for ds in so
            ] for so in scheduler_output_per_rank
        ]
        scheduler_output_bwd = create_random_dispatch_from_existing(per_rank_shard_lens, world_size)
        return scheduler_output, glob_doc_lens, scheduler_output_bwd
    else:
        return scheduler_output, glob_doc_lens


def _block_reverse_list(l: list, d: int):
    """
    Blockwise reverse a list:
    return l[-d:0] + l[-2d:-d] + ...
    This is because the backward is the flip of forward in the pp dimension, but
    keep the order in the dp dimension.
    """
    return [item for i in range(len(l), 0, -d) for item in l[max(0, i - d):i]]


from global_batch_provider import get_next_batch, GLOBAL_BATCH
def create_pipeline_doclens(
    ref_doc_lens: Optional[list[list[int]]],
    add_dummy: bool,
    is_backward: bool,
    world_size: int,
    total_token_on_rank: int,
    num_docs: int,
    tp_size: int,
    dp_size: int,
    num_batches: int = None,
    use_planner: bool = False,
    return_original_doclen: bool = False,
) -> list[list[int]]: # list of batch[int]
    """
    Create `num_batches` batch for a microbatch (one pp-tick).
    - Take a batch from get_next_batch (or random) and also handle some padding logic.

    For a forward tick, its sequence length follows:
        [new_microbatch, last_tick_seq_len[:PP_stage_-1]]

        The new_microbatch is either generated, or a dummy one. (controlled by add_dummy)
        A special case is that, the first tick does not have a previous one.
        In this way, we make all stages' microbatch dummy, except for the first one.

    For a backward tick, its sequence length is the reverse of a forward tick.
    Args:
        ref_shard_lens: None only if this is the first tick in PP. Otherwise, return the shard_lens of last tick.
        add_dummy: add dummy forward microbatches for those pp_ranks == 0 devices
        is_backward: this is a backward seqlens. In this case, it directly flips the seqlens of a corresponding forward
    """
    if is_backward:
        return _block_reverse_list(ref_doc_lens, dp_size)
    # Create new microbatches for the first PP stage
    if add_dummy:
        # Each rank only gets one dummy document, which is of `tp_size`
        # to avoid Sequence Parallel having an issue.
        pp_head_new_doc_len = [[total_token_on_rank // dp_size] for _ in range(dp_size)]  # FIXME: using total_token_on_rank for cudagraph to work out-of-box, can be tp_size when cudagraph is disabled.
    else:
        assert total_token_on_rank % tp_size == 0, "Sequence Parallel requires total token divisible by tp_size"
        # TODO: Do not do "use_planner" to decide if we should grab batch from global batch or random.
        if use_planner == False:
            pp_head_new_doc_len = [
                create_list(random.randint(1, num_docs), total_token_on_rank, tp_size, 1)
                for _ in range(dp_size)
            ]
        else:
            # we need to use GLOBAL_BATCH to get dp number of list.
            #from test_megatron_e2e_pipeline_planner import GLOBAL_BATCH, get_next_batch
            print(f"In util.py, before calling get_next_batch: GLOBAL_BATCH is: {GLOBAL_BATCH}")
            num_batches = num_batches or dp_size 
            pp_head_new_doc_len = get_next_batch(num_batches)
    print(f"In util.py, after calling get_next_batch: pp_head_new_doc_len is: {pp_head_new_doc_len}")

    #  pp_head_new_doc_len : shape : [dp, num_seqs]  We should sample seq_len here.
    # And add the sampled seq_len to the batch. 
    # Next step is based on the previous batch, move the batch. 
    # Get existing microbatch seqlens
    if ref_doc_lens is not None:
        if num_batches == None:
            num_batches = dp_size
        # Not the first microbatch
        if ref_doc_lens[-1] == [tp_size]:
            other_pp_doc_len = ref_doc_lens[:-dp_size]
        else:
            other_pp_doc_len = ref_doc_lens[:-num_batches]
    else:
        dummy_fwd_num = world_size - dp_size
        other_pp_doc_len = [[total_token_on_rank // dp_size] for _ in range(dummy_fwd_num)]  # FIXME: using total_token_on_rank for cudagraph to work out-of-box, can be tp_size when cudagraph is disabled.
    tick_per_rank_doc_len = pp_head_new_doc_len + other_pp_doc_len
    print(f"In util.py, finally tick_per_rank_doc_len is: {tick_per_rank_doc_len}")

    if return_original_doclen:
        print("In util.py, return_original_doclen is True, returning (tick_per_rank_doc_len, pp_head_new_doc_len)")
        return (tick_per_rank_doc_len, pp_head_new_doc_len)
    # exit(0)
    return tick_per_rank_doc_len


def random_tick_shard_from_doclens(
    per_rank_doc_lens: list[list[int]],
    tp_size: int,   # control the max num shard
    max_cp_degree: int,
):
    world_size = len(per_rank_doc_lens)
    per_rank_shard_lens = []
    for r in range(world_size):
        docs = per_rank_doc_lens[r]
        rank_shards = []
        for doc_len in docs:
            num_shards = min(random.randint(1, max_cp_degree), doc_len // tp_size)
            assert doc_len >= tp_size, f"{r, doc_len, tp_size}"
            rank_shards.append(create_list(num_shards, doc_len, tp_size, 1))
        per_rank_shard_lens.append(rank_shards)
    return create_random_dispatch_from_existing(
        per_rank_shard_lens, world_size,
    )

from d2.planner.planner import Planner
from types import SimpleNamespace

# In this function, world size is actually as_world_size.
def create_qkv_dispatch_pipeline_tick(
    world_size: int, total_num_token: int, num_docs: int, max_cp_degree: int,
    hidden_size_q: int, hidden_size_k: int,
    element_size: int, # dtype's size
    softmax_lse_size: int,
    ref_doc_lens: Optional[torch.Tensor],
    add_dummy: bool,
    tp_size: int, dp_size: int,
    num_token_per_rank: int,
    num_batches: int = None,
    use_planner: bool = False,
    return_original_doclen: bool = False,
):
    """
    softmax_lse_size (int): size of the softmax_lse tensor when viewed as the dtype,
        should be num_heads_local * fp32.itemsize // element_size.
    """
    print(f"游리 Inside create_qkv_dispatch_pipeline_tick: {world_size = }. Please check the world size as this is acutlaly should just be dp * pp.")
    create_pp_doclen_kwargs = dict(
        world_size=world_size,
        total_token_on_rank=total_num_token,
        num_docs=num_docs,
        tp_size=tp_size,
        dp_size=dp_size,
    )

    # Calls `get_next_batch` if use_planner is True and get the new sequence lens.
    cur_tick_per_rank_doc_lens, original_cur_tick_per_rank_doc_lens = create_pipeline_doclens(
        ref_doc_lens, add_dummy, is_backward=False, num_batches = num_batches, use_planner=use_planner,
        **create_pp_doclen_kwargs, return_original_doclen=True,
    )
    assert isinstance(cur_tick_per_rank_doc_lens, list), f"cur_tick_per_rank_doc_lens: {cur_tick_per_rank_doc_lens} is not a list"
    print(f"游리 cur_tick_per_rank_doc_lens: {cur_tick_per_rank_doc_lens}, len(cur_tick_per_rank_doc_lens): {len(cur_tick_per_rank_doc_lens)}")
    print(f"游리 original_cur_tick_per_rank_doc_lens: {original_cur_tick_per_rank_doc_lens}, len(original_cur_tick_per_rank_doc_lens): {len(original_cur_tick_per_rank_doc_lens)}")


    # Try different tolerance_factor values to find one that passes the buffer size check
    tolerance_factors_to_try = [0.05, 0.1, 0.15, 0.2, 0.22, 0.25, 0.28, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    MIN_TOLERANCE_FACTOR = 0.05
    try:
        MIN_TOLERANCE_FACTOR = os.environ.get("MIN_TOLERANCE_FACTOR", "0.05")
        MIN_TOLERANCE_FACTOR = float(MIN_TOLERANCE_FACTOR)
    except ValueError:
        pass
    print(f"游리 MIN_TOLERANCE_FACTOR = {MIN_TOLERANCE_FACTOR}")

    tolerance_factors_to_try.append(MIN_TOLERANCE_FACTOR)
    tolerance_factors_to_try = set(tolerance_factors_to_try)
    tolerance_factors_to_try = list(tolerance_factors_to_try)
    tolerance_factors_to_try.sort()
    
    for tolerance_factor in tolerance_factors_to_try:
        if tolerance_factor < MIN_TOLERANCE_FACTOR:
            continue
        print(f"游리 Trying tolerance_factor: {tolerance_factor}")
        
        start_time = time.time()
        if use_planner:
            # TODO: This pp_size should really come from mpu or some module that handles the world size...
            pp_size = world_size // dp_size        # in this function, world size is actual as_world_size. pp = world_size // dp_size
            planner = Planner.from_individual_params(
                tp_size=tp_size,
                pp_size=pp_size,
                dp_size=dp_size,
                world_size=tp_size * pp_size * dp_size,
                hidden_size_q=hidden_size_q,
                hidden_size_k=hidden_size_k
            )
            planner.tolerance_factor = tolerance_factor
            
            # Create a temp_model_config for item initialization.
            temp_model_config = SimpleNamespace(
                hidden_size=hidden_size_q * tp_size,
                num_attention_heads = 1,
                num_key_value_heads = hidden_size_q // hidden_size_k,
                num_hidden_layers = 1
            )
            # Use batch_to_items_with_dummy to handle dummy doc to item.
            items = batch_to_items_with_dummy(batches=cur_tick_per_rank_doc_lens, 
                                              num_tokens_per_rank = num_token_per_rank,
                                              as_world_size=world_size,
                                              model_config=temp_model_config)
            fwd_planner_out = planner.items_to_shardinfo(items)
        else:
            fwd_planner_out = random_tick_shard_from_doclens(
                cur_tick_per_rank_doc_lens, tp_size, max_cp_degree,
            )


        fa2a_metadata = from_planner_output(
            world_size, fwd_planner_out, hidden_size_q, hidden_size_k,
            softmax_lse_size, element_size, is_pipeline_tick=True
        )
        (qkv_linear_to_attn_fa2a, _, out_attn_to_linear_fa2a, _, fwd_attn_metadata) = fa2a_metadata
        end_time = time.time()
        print(f"游리 游리 create_qkv_dispatch_pipeline_tick - fwd_planner_out duration: {(end_time - start_time):.2f} sec", flush=True)

        
        # CP flip logic.
        start_time = time.time()
        print(f"游리 fwd_tick_per_rank_doc_lens: {cur_tick_per_rank_doc_lens}")
        if len(cur_tick_per_rank_doc_lens) < world_size:
            print("游리 CP flip logic.")
            bwd_tick_per_rank_doc_lens = _block_reverse_list(cur_tick_per_rank_doc_lens, num_batches)
        else:
            # None CP flip logic.
            print("游리 None CP flip logic.")
            bwd_tick_per_rank_doc_lens = create_pipeline_doclens(
                cur_tick_per_rank_doc_lens, add_dummy=False, is_backward=True,
                **create_pp_doclen_kwargs,
            )
        print(f"游리 bwd_tick_per_rank_doc_lens: {bwd_tick_per_rank_doc_lens}")
        
        if use_planner:
            items = batch_to_items_with_dummy(batches=bwd_tick_per_rank_doc_lens, 
                                              num_tokens_per_rank=num_token_per_rank,
                                              as_world_size=world_size,
                                              model_config=temp_model_config)
            bwd_planner_out = planner.items_to_shardinfo(items)
        else:
            bwd_planner_out = random_tick_shard_from_doclens(
                bwd_tick_per_rank_doc_lens, tp_size, max_cp_degree,
            )
        (qkv_resend_and_out_grad_linear_to_attn_fa2a, qkv_grad_attn_to_linear_fa2a,
         bwd_attn_metadata) = backward_from_planner_output(
             world_size, bwd_planner_out, hidden_size_q, hidden_size_k,
             softmax_lse_size, element_size,
        )


        ret = (fwd_attn_metadata, bwd_attn_metadata,
                qkv_linear_to_attn_fa2a, qkv_grad_attn_to_linear_fa2a,
                out_attn_to_linear_fa2a, qkv_resend_and_out_grad_linear_to_attn_fa2a,
                cur_tick_per_rank_doc_lens,)
        if return_original_doclen:
            ret += (original_cur_tick_per_rank_doc_lens,)

        end_time = time.time()
        print(f"游리 游리 create_qkv_dispatch_pipeline_tick - cp flip duration: {(end_time - start_time):.2f} sec")

        # FIXME: Properly pass the output dir down here.
        start_time = time.time()
        from d2.utils.network_inspect import inspect_network_metadata
        output_dir = os.environ.get("EXPERIMENT_OUTPUT_DIR")
        if torch.distributed.is_initialized():
            rank = torch.distributed.get_rank()
        else:
            # NOTE: This can potentially cause race condition. Mindful!
            # rank = 0 
            rank = 1 # so we don't write anything into the log file, but still print the result to console.
        
        # FIXME: Check the network metadata logic here.
        network_metadata = (
            qkv_linear_to_attn_fa2a, qkv_resend_and_out_grad_linear_to_attn_fa2a,
            out_attn_to_linear_fa2a, qkv_grad_attn_to_linear_fa2a,
        )


        sample_id = os.environ.get("__PRG__INTERNAL__EXPERIMENT_SAMPLE_ID", "0")
        network_buffer_requirements = inspect_network_metadata(
            network_metadata, is_ping=None, sample_id=sample_id, 
            tolerance_factor=tolerance_factor, 
            output_dir=output_dir, rank=rank,
            seq_len=cur_tick_per_rank_doc_lens,
        )
        end_time = time.time()
        print(f"游리 游리 create_qkv_dispatch_pipeline_tick - network_inspect duration: {(end_time - start_time):.2f} sec")

        def debug_set_metadata_transfer_size_to_0(
            qkv_fwd_metadata,
            qkv_bwd_metadata,
            attn_out_fwd_metadata,
            attn_out_bwd_metadata,
        ):
            for param in [
                qkv_fwd_metadata,
                qkv_bwd_metadata,
                attn_out_fwd_metadata,
                attn_out_bwd_metadata,
            ]:
                param.fa2a_metadata[1][:] = 1
                param.fa2a_metadata[3][:] = 1
                param.my_rank_send_sz = 1
            return
        
        
        if os.environ.get("EXPERIMENT_DEBUG_SET_METADATA_TRANSFER_SIZE_TO_0", "0") == "1":
            print(f"游리 [Rank {rank}] Debug set metadata transfer size to 0")
            debug_set_metadata_transfer_size_to_0(
                qkv_linear_to_attn_fa2a, qkv_resend_and_out_grad_linear_to_attn_fa2a,
                out_attn_to_linear_fa2a, qkv_grad_attn_to_linear_fa2a,
            )


        def is_buffer_size_enough(buffer_size, network_buffer_requirements):
            max_buffer_budget_all_rank = network_buffer_requirements["max_buffer_budget_all_rank"]
            print(f"游리 Got max_buffer_budget_all_rank = {max_buffer_budget_all_rank / 1024 ** 3} GB ({max_buffer_budget_all_rank}), buffer_size = {buffer_size / 1024 ** 3} GB ({buffer_size}). Is enough: {(buffer_size >= max_buffer_budget_all_rank) = }")

            return buffer_size >= max_buffer_budget_all_rank

        from d2.runtime.attn_kernels.ops import DispatcherWrapper
        buffer_size = DispatcherWrapper.instance[0].buffer_size
        
        # Check if current tolerance_factor works
        if is_buffer_size_enough(buffer_size, network_buffer_requirements):
            print(f"游리 Found working tolerance_factor: {tolerance_factor}")
            return ret
        else:
            print(f"游리 tolerance_factor {tolerance_factor} failed: buffer_size={buffer_size}, required={network_buffer_requirements['max_buffer_budget_all_rank']}")
    
    # If we reach here, none of the tolerance_factors worked
    raise ValueError(f"None of the tolerance factors {tolerance_factors_to_try} resulted in sufficient buffer size. "
                     f"Buffer size: {buffer_size}, latest requirements: {network_buffer_requirements}. "
                     f"You may need to increase the buffer size or try larger tolerance factors.")


######## TODO: deprecate all below
def gen_seq_lens(world_size: int, num_seqs: int, total_len: int) -> torch.Tensor:
    ratio = torch.rand((world_size, num_seqs)) + 0.25 / num_seqs   # Use a min value to guarantee that the sequence is not too short (0 after rounding)
    ratio = ratio / ratio.sum(dim=1, keepdim=True)
    seq_len = (ratio * total_len).round().int()
    seq_len_total = seq_len.sum(dim=1)
    seq_len_total_error = seq_len_total - total_len
    seq_len[:, -1] -= seq_len_total_error
    return seq_len
