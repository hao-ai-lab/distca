import json
import torch
import flashinfer
import gc
import numpy as np
import multiprocessing as mp
from pathlib import Path
from rich.progress import Progress, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn
from rich.console import Console
from rich import print as rprint

filename = 'cp_shardq2'
result_path = Path(__file__).parent.parent / 'results' / f'{filename}.jsonl'
result_path.parent.mkdir(parents=True, exist_ok=True)


K = 1024
M = K * K

def get_mask(q_length, kv_length, rank, batch_size):
    assert q_length % 2 == 0
    q_length = q_length // 2
    a = torch.zeros((q_length, kv_length), dtype=torch.bool)
    b = torch.ones((q_length, kv_length), dtype=torch.bool)

    # Upper
    for i in range(q_length):
        right = rank * q_length + i + 1
        a[i, :right] = True
    for i in range(q_length):
        start = kv_length - q_length * (rank+1) + i + 1
        # print(start)
        b[i, start:] = False
        pass
    # concat a, b 
    c = torch.cat([a, b], dim=0)
    # replicate c `batch_size` times
    d = torch.cat([c] * batch_size, dim=0)
    return d

def visualize_mask_heatmap(mask):
    import matplotlib.pyplot as plt
    plt.imshow(mask.cpu().float(), cmap='viridis', aspect='auto')
    plt.colorbar()
    plt.title("Custom Attention Mask")
    plt.show()

def run_flash_attention(rank, batch_size, qo_len, kv_len, num_qo_heads, num_kv_heads, head_dim, device, queue):
    try:
        q = torch.randn(qo_len * batch_size, num_qo_heads, head_dim, device=device, dtype=torch.float16)
        print(f"q.shape: {q.shape}, q size: {q.numel() * q.element_size() / 1024 ** 3} GB")
        k = torch.randn(kv_len, num_kv_heads, head_dim, device=device, dtype=torch.float16)
        print(f"k.shape: {k.shape}, k size: {k.numel() * k.element_size() / 1024 ** 3} GB")
        v = torch.randn(kv_len, num_kv_heads, head_dim, device=device, dtype=torch.float16)
        print(f"v.shape: {v.shape}, v size: {v.numel() * v.element_size() / 1024 ** 3} GB")
        mask = get_mask(qo_len, kv_len, rank, batch_size).to(device)
        print(f"mask.shape: {mask.shape}, mask size: {mask.numel() * mask.element_size() / 1024 ** 3} GB")
        compute_times = []

        for _ in range(10):
            start_event = torch.cuda.Event(enable_timing=True)
            end_event = torch.cuda.Event(enable_timing=True)
            start_event.record()
            o_custom = flashinfer.single_prefill_with_kv_cache(q, k, v, custom_mask=mask)
            end_event.record()
            torch.cuda.synchronize()
            print(f"o_custom.shape: {o_custom.shape}, o_custom size: {o_custom.numel() * o_custom.element_size() / 1024 ** 3} GB")
            compute_times.append(start_event.elapsed_time(end_event))
            torch.cuda.empty_cache()

        median_time = np.median(compute_times)
        result = dict(
            rank=rank,
            batch_size=batch_size,
            qo_len=qo_len,
            kv_len=kv_len,
            num_qo_heads=num_qo_heads,
            num_kv_heads=num_kv_heads,
            head_dim=head_dim,
            median_compute_time=median_time,
            all_compute_times=compute_times,
        )
        queue.put(result)
    except Exception as e:
        queue.put({"rank": rank, "error": str(e)})
    finally:
        gc.collect()
        torch.cuda.empty_cache()

def safe_run_flash_attention(rank, batch_size, qo_len, kv_len, num_qo_heads, num_kv_heads, head_dim=128, device="cuda"):
    queue = mp.Queue()
    p = mp.Process(
        target=run_flash_attention,
        args=(rank, batch_size, qo_len, kv_len, num_qo_heads, num_kv_heads, head_dim, device, queue)
    )
    p.start()
    p.join(timeout=60) # seconds

    if p.is_alive():
        p.terminate()
        return {"rank": rank, "error": "timeout"}

    return queue.get()

# Example batch run
if __name__ == "__main__":
    mp.set_start_method("spawn")  # Safe for CUDA
    cp_degrees = [4, 8, 2, 1]
    # qo_lens = [8, 16, 32, 64, 128, 256]
    qo_lens = [512, 1024]
    batch_sizes = [1, 2, 4, 8]
    result = []

    configs = dict(
        llama8b = dict(
            num_qo_heads=32,
            num_kv_heads=8,
            head_dim=128,
        ),
        llama70b = dict(
            num_qo_heads=64,
            num_kv_heads=8,
            head_dim=128,
        )
    )
    
    # Load existing results
    completed_runs = set()
    if result_path.exists():
        with open(result_path, "r") as f:
            for line in f:
                item = json.loads(line)
                key = (
                    item['cp_degree'], item['rank'], item['batch_size'], item['qo_len'],
                    item['num_qo_heads'], item['num_kv_heads'], item['head_dim']
                )
                completed_runs.add(key)
    print(f"Completed runs: {completed_runs}")

    total_tasks = len(batch_sizes) * len(qo_lens) * sum([
        cp_degree for cp_degree in cp_degrees
    ])
    console = Console()

    with Progress(
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TaskProgressColumn(),
        TimeRemainingColumn(),
        console=console,
    ) as progress:
        task = progress.add_task("[cyan]Running flash attention benchmarks...", total=total_tasks)
        for name, config in configs.items():
            for cp_degree in cp_degrees:
                for batch_size in batch_sizes:
                    for qo_len in qo_lens:
                        # # for rank in range(cp_degree):
                        for rank in range(1):
                            # Skip if already computed
                            num_qo_heads = config['num_qo_heads']
                            num_kv_heads = config['num_kv_heads']
                            head_dim = config['head_dim']

                            adjusted_qo_len = K * (qo_len // cp_degree)
                            kv_len = K * qo_len

                            key = (
                                cp_degree, rank, batch_size, adjusted_qo_len,
                                num_qo_heads, num_kv_heads, head_dim,
                            )
                            if key in completed_runs:
                                progress.console.print(f"[yellow]Skipping completed run: cp_degree={cp_degree}, rank={rank}, batch_size={batch_size}, qo_len={qo_len}[/yellow]")
                                progress.advance(task)
                                continue
                            print(f"key: {key} not in completed_runs")

                            
                            res = safe_run_flash_attention(
                                rank, batch_size, adjusted_qo_len, kv_len, 
                                num_qo_heads, num_kv_heads, head_dim,
                            )
                            progress.advance(task)
                            if "error" in res:
                                rprint(f"[red]Error in cp_degree={cp_degree}, rank={rank}, batch_size={batch_size}, qo_len={qo_len}: {res['error']}[/red]")
                                continue

                            rprint(f"[green]cp_degree={cp_degree}, rank={rank}, batch_size={batch_size}, qo_len={qo_len}: {res['median_compute_time']:.2f} ms[/green]")
                        
                            # Add cp_degree to result
                            res['cp_degree'] = cp_degree
                            result.append(res)

                            with open(result_path, "a") as f:
                                json.dump(res, f)
                                f.write("\n")

                        