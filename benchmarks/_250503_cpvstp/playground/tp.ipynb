{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import flashinfer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device('cuda:3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(q_length, kv_length, rank, batch_size):\n",
    "    a = torch.tril(torch.ones(q_length, kv_length))\n",
    "    b = torch.cat([a] * batch_size, dim=0)\n",
    "    return b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_flash_attention(rank=0, batch_size=1, qo_len=128, kv_len=4096, num_qo_heads=32, num_kv_heads=32, head_dim=128, repeat=7, visualize_mask=False,device=\"cuda\",return_tensors=False,verbose=False):\n",
    "    def print_if_verbose(s):\n",
    "        if verbose:\n",
    "            print(s)\n",
    "        return\n",
    "    \n",
    "    print_if_verbose(f\"Running flash attention with rank {rank}, batch size {batch_size}, qo_len {qo_len}, kv_len {kv_len}, num_qo_heads {num_qo_heads}, num_kv_heads {num_kv_heads}, head_dim {head_dim}, visualize_mask {visualize_mask}, device {device}, verbose {verbose}\")\n",
    "    q = torch.randn(qo_len * batch_size, num_qo_heads, head_dim).half().to(device)\n",
    "    k = torch.randn(kv_len, num_kv_heads, head_dim).half().to(device)\n",
    "    v = torch.randn(kv_len, num_kv_heads, head_dim).half().to(device)\n",
    "    mask = get_mask(qo_len, kv_len, rank, batch_size)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    compute_times = []\n",
    "    for _ in range(repeat):\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        start_event.record()\n",
    "        o_custom = flashinfer.single_prefill_with_kv_cache(q, k, v, custom_mask=mask)\n",
    "        end_event.record()\n",
    "\n",
    "        # Waits for everything to finish running\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "        compute_times.append(elapsed_time_ms)\n",
    "        print_if_verbose(f\"Elapsed time: {elapsed_time_ms:.2f} ms\")\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    median_compute_time = torch.tensor(compute_times).median()\n",
    "    return_values = [None, compute_times, median_compute_time]\n",
    "    if return_tensors:\n",
    "        return_values[0] = o_custom\n",
    "    return return_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "tp_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 10, computed_time: 2.35\n",
      "k: 11, computed_time: 0.29\n",
      "k: 12, computed_time: 0.93\n",
      "k: 13, computed_time: 2.41\n",
      "k: 14, computed_time: 13.29\n",
      "k: 15, computed_time: 33.12\n",
      "k: 16, computed_time: 130.45\n",
      "Error: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 14.05 GiB is free. Including non-PyTorch memory, this process has 65.07 GiB memory in use. Of the allocated memory 64.59 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Config: {'rank': 0, 'batch_size': 1, 'qo_len': 131072, 'kv_len': 131072, 'num_qo_heads': 8, 'num_kv_heads': 2, 'head_dim': 128}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "configs = dict(\n",
    "    llama8b=dict(\n",
    "        num_qo_heads=32,\n",
    "        num_kv_heads=8,\n",
    "        head_dim=128,\n",
    "    ),\n",
    "    llama70b=dict(\n",
    "        num_qo_heads=64,\n",
    "        num_kv_heads=8,\n",
    "        head_dim=128,\n",
    "    )\n",
    ")\n",
    "\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for name, model_config in configs.items():\n",
    "        for k in range(10, 20+ 1):\n",
    "            qo_len = kv_len = 2 ** k\n",
    "            config = dict(\n",
    "                rank=0,\n",
    "                batch_size=1,\n",
    "                qo_len=qo_len,\n",
    "                kv_len=kv_len,\n",
    "                num_qo_heads=model_config['num_qo_heads'] // tp_size,\n",
    "                num_kv_heads=model_config['num_kv_heads'] // tp_size,\n",
    "                head_dim=model_config['head_dim'],\n",
    "            )\n",
    "            try:\n",
    "                proc = Process(\n",
    "                    target=run_flash_attention, \n",
    "                    kwargs=dict(\n",
    "                        **config,\n",
    "                        repeat=7,\n",
    "                        return_tensors=False,\n",
    "                    )\n",
    "                )\n",
    "                item = run_flash_attention(\n",
    "                    **config,\n",
    "                    repeat=7,\n",
    "                    return_tensors=False,\n",
    "                )\n",
    "                config['tp_size'] = tp_size\n",
    "                computed_time = item[-1]\n",
    "                print(f\"k: {k}, computed_time: {computed_time:.2f}\")\n",
    "                \n",
    "                config_named_tuple = namedtuple('Config', config.keys())\n",
    "                config_named_tuple(**config)\n",
    "                results[config_named_tuple] = computed_time\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                print(f\"Config: {config}\")\n",
    "                raise e\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
