{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import flashinfer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mask(mask, use_unicode=True):\n",
    "    \"\"\"\n",
    "    Visualize a 2D or 4D boolean attention mask in ASCII.\n",
    "    For 4D: (batch, head, seq, seq) -> shows each (head, seq, seq) block.\n",
    "    \"\"\"\n",
    "    if mask.dim() == 4:\n",
    "        B, H, N, M = mask.shape\n",
    "        for b in range(B):\n",
    "            for h in range(H):\n",
    "                print(f\"Batch {b}, Head {h}\")\n",
    "                visualize_mask(mask[b, h], use_unicode)\n",
    "                print()\n",
    "    elif mask.dim() == 2:\n",
    "        N, M = mask.shape\n",
    "        for i in range(N):\n",
    "            line = \"\"\n",
    "            for j in range(M):\n",
    "                if use_unicode:\n",
    "                    line += \"✅\" if mask[i, j] else \"❌\"\n",
    "                else:\n",
    "                    line += \".\" if mask[i, j] else \"X\"\n",
    "            print(line)\n",
    "    else:\n",
    "        raise ValueError(\"Mask must be 2D or 4D boolean tensor\")\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_mask_heatmap(mask, figsize=(6, 6), cmap=\"Greys\"):\n",
    "    \"\"\"\n",
    "    Visualize a 2D or 4D boolean attention mask as a heatmap using matplotlib.\n",
    "    For 4D: (batch, head, seq, seq) -> shows a grid of subplots.\n",
    "    \"\"\"\n",
    "    if mask.dim() == 4:\n",
    "        B, H, N, M = mask.shape\n",
    "        for b in range(B):\n",
    "            fig, axes = plt.subplots(1, H, figsize=(figsize[0] * H, figsize[1]))\n",
    "            fig.suptitle(f\"Batch {b}\", fontsize=14)\n",
    "            for h in range(H):\n",
    "                ax = axes[h] if H > 1 else axes\n",
    "                ax.imshow(mask[b, h].cpu().float(), cmap=cmap, interpolation=\"nearest\")\n",
    "                ax.set_title(f\"Head {h}\")\n",
    "                ax.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    elif mask.dim() == 2:\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(mask.cpu().float(), cmap=cmap, interpolation=\"nearest\")\n",
    "        plt.title(\"Attention Mask\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Mask must be 2D or 4D boolean tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(q_length, kv_length, rank, batch_size):\n",
    "    a = torch.zeros((q_length, kv_length), dtype=torch.bool)\n",
    "    b = torch.ones((q_length, kv_length), dtype=torch.bool)\n",
    "\n",
    "    # Upper\n",
    "    for i in range(q_length):\n",
    "        right = rank * q_length + i + 1\n",
    "        a[i, :right] = True\n",
    "    for i in range(q_length):\n",
    "        start = kv_length - q_length * (rank+1) + i + 1\n",
    "        # print(start)\n",
    "        b[i, start:] = False\n",
    "        pass\n",
    "    # concat a, b \n",
    "    c = torch.cat([a, b], dim=0)\n",
    "    # replicate c `batch_size` times\n",
    "    d = torch.cat([c] * batch_size, dim=0)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_flash_attention(rank=0, batch_size=1, qo_len=128, kv_len=4096, num_qo_heads=32, num_kv_heads=32, head_dim=128, repeat=7, visualize_mask=False,device=\"cuda\",return_tensors=False,verbose=False):\n",
    "    def print_if_verbose(s):\n",
    "        if verbose:\n",
    "            print(s)\n",
    "    \n",
    "    print_if_verbose(f\"Running flash attention with rank {rank}, batch size {batch_size}, qo_len {qo_len}, kv_len {kv_len}, num_qo_heads {num_qo_heads}, num_kv_heads {num_kv_heads}, head_dim {head_dim}, visualize_mask {visualize_mask}, device {device}, verbose {verbose}\")\n",
    "    q = torch.randn(qo_len * batch_size, num_qo_heads, head_dim).half().to(device)\n",
    "    k = torch.randn(kv_len, num_kv_heads, head_dim).half().to(device)\n",
    "    v = torch.randn(kv_len, num_kv_heads, head_dim).half().to(device)\n",
    "    mask = get_mask(qo_len // 2, kv_len, rank, batch_size)\n",
    "    mask = mask.to(device)\n",
    "    if visualize_mask:\n",
    "        visualize_mask_heatmap(mask)\n",
    "    if verbose:\n",
    "        print(f\"q: {q.shape}, k: {k.shape}, v: {v.shape}, mask: {mask.shape}\")\n",
    "\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    compute_times = []\n",
    "    for _ in range(repeat):\n",
    "        start_event.record()\n",
    "        o_custom = flashinfer.single_prefill_with_kv_cache(q, k, v, custom_mask=mask)\n",
    "        end_event.record()\n",
    "\n",
    "        # Waits for everything to finish running\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "        compute_times.append(elapsed_time_ms)\n",
    "        print_if_verbose(f\"Elapsed time: {elapsed_time_ms:.2f} ms\")\n",
    "    \n",
    "    return_values = [None, None, compute_times]\n",
    "    if return_tensors:\n",
    "        return_values[0] = o_custom\n",
    "        return_values[1] = mask\n",
    "\n",
    "    return return_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running flash attention with rank 0, batch size 2, qo_len 1024, kv_len 1024, num_qo_heads 32, num_kv_heads 32, head_dim 128, visualize_mask True, device cuda, verbose True\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: torch.Size([2048, 32, 128]), k: torch.Size([1024, 32, 128]), v: torch.Size([1024, 32, 128]), mask: torch.Size([2048, 1024])\n",
      "Elapsed time: 0.94 ms\n",
      "Elapsed time: 0.54 ms\n",
      "Elapsed time: 0.40 ms\n",
      "Elapsed time: 0.39 ms\n",
      "Elapsed time: 0.39 ms\n",
      "Elapsed time: 0.38 ms\n",
      "Elapsed time: 0.38 ms\n"
     ]
    }
   ],
   "source": [
    "K = 1024\n",
    "M = K * K\n",
    "run_flash_attention(\n",
    "    rank=0, \n",
    "    batch_size=2,\n",
    "    qo_len=1 * K, \n",
    "    kv_len=1 * K,\n",
    "    # kv_len=2 * K,\n",
    "    num_qo_heads=32, num_kv_heads=32,\n",
    "    # visualize_mask=False,\n",
    "    visualize_mask=True,\n",
    "    verbose=True,\n",
    ")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m qo_len \u001b[38;5;129;01min\u001b[39;00m qo_lens:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m rank \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cp_degree):    \n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         result_item = run_flash_attention(\n\u001b[32m      8\u001b[39m             rank=rank, batch_size=\u001b[32m1\u001b[39m,\n\u001b[32m      9\u001b[39m             qo_len=K * (\n\u001b[32m     10\u001b[39m                 qo_len // \u001b[32m2\u001b[39m // cp_degree\n\u001b[32m     11\u001b[39m             ), \n\u001b[32m     12\u001b[39m             kv_len=K * qo_len,\n\u001b[32m     13\u001b[39m             num_qo_heads=\u001b[32m32\u001b[39m, \n\u001b[32m     14\u001b[39m             num_kv_heads=\u001b[32m8\u001b[39m,\n\u001b[32m     15\u001b[39m         )\n\u001b[32m     16\u001b[39m         compute_times = result_item[\u001b[32m2\u001b[39m]\n\u001b[32m     17\u001b[39m         median_compute_time = np.median(compute_times)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mrun_flash_attention\u001b[39m\u001b[34m(rank, batch_size, qo_len, kv_len, num_qo_heads, num_kv_heads, head_dim, repeat, visualize_mask, device, return_tensors, verbose)\u001b[39m\n\u001b[32m      4\u001b[39m         \u001b[38;5;28mprint\u001b[39m(s)\n\u001b[32m      6\u001b[39m print_if_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning flash attention with rank \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, qo_len \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqo_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, kv_len \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkv_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, num_qo_heads \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_qo_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, num_kv_heads \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_kv_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, head_dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, visualize_mask \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvisualize_mask\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, verbose \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mverbose\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m q = torch.randn(qo_len * batch_size, num_qo_heads, head_dim).half().to(device)\n\u001b[32m      8\u001b[39m k = torch.randn(kv_len, num_kv_heads, head_dim).half().to(device)\n\u001b[32m      9\u001b[39m v = torch.randn(kv_len, num_kv_heads, head_dim).half().to(device)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "cp_degree = 2\n",
    "# qo_lens = [128, 256]\n",
    "# qo_lens = [64, 32, 16, 8,]\n",
    "qo_lens = [4, 2, 1]\n",
    "for qo_len in qo_lens:\n",
    "    for rank in range(cp_degree):    \n",
    "        result_item = run_flash_attention(\n",
    "            rank=rank, batch_size=1,\n",
    "            qo_len=K * (\n",
    "                qo_len // 2 // cp_degree\n",
    "            ), \n",
    "            kv_len=K * qo_len,\n",
    "            num_qo_heads=32, \n",
    "            num_kv_heads=8,\n",
    "        )\n",
    "        compute_times = result_item[2]\n",
    "        median_compute_time = np.median(compute_times)\n",
    "        print(f\"Rank {rank} qo_len {qo_len} median compute time: {median_compute_time:.2f} ms\")\n",
    "        result.append({\n",
    "            \"cp_degree\": cp_degree,\n",
    "            \"rank\": rank,\n",
    "            \"qo_len\": qo_len,\n",
    "            \"median_compute_time\": median_compute_time,\n",
    "            \"all_compute_times\": compute_times,\n",
    "        })\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
