{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-235B-A22B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AutoConfig.from_pretrained(model_id, trust_remote_code=True)  # 94 layers originally  [oai_citation:0‡Hugging Face](https://huggingface.co/Qwen/Qwen3-235B-A22B/blob/main/config.json?utm_source=chatgpt.com)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.num_hidden_layers = 1          # ← the only change strictly required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3MoeConfig {\n",
       "  \"architectures\": [\n",
       "    \"Qwen3MoeForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"decoder_sparse_step\": 1,\n",
       "  \"eos_token_id\": 151645,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 12288,\n",
       "  \"max_position_embeddings\": 40960,\n",
       "  \"max_window_layers\": 94,\n",
       "  \"mlp_only_layers\": [],\n",
       "  \"model_type\": \"qwen3_moe\",\n",
       "  \"moe_intermediate_size\": 1536,\n",
       "  \"norm_topk_prob\": true,\n",
       "  \"num_attention_heads\": 64,\n",
       "  \"num_experts\": 128,\n",
       "  \"num_experts_per_tok\": 8,\n",
       "  \"num_hidden_layers\": 1,\n",
       "  \"num_key_value_heads\": 4,\n",
       "  \"output_router_logits\": false,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"router_aux_loss_coef\": 0.001,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 151936\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda\")                # PyTorch 2.1+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)              # reproducible randomness\n",
    "with torch.no_grad():\n",
    "    model = AutoModelForCausalLM.from_config(cfg, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3MoeForCausalLM(\n",
       "  (model): Qwen3MoeModel(\n",
       "    (embed_tokens): Embedding(151936, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0): Qwen3MoeDecoderLayer(\n",
       "        (self_attn): Qwen3MoeAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3MoeRMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MoeSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=128, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-127): 128 x Qwen3MoeMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (down_proj): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): Qwen3MoeRMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3MoeRMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3MoeRMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3MoeRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers.modeling_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /global/homes/j/jundac/.cache/huggingface/hub/models--Qwen--Qwen3-235B-A22B/snapshots/c30ce1aa8a0ff9cebf95e95b4b8fd90826043fd0/vocab.json\n",
      "loading file merges.txt from cache at /global/homes/j/jundac/.cache/huggingface/hub/models--Qwen--Qwen3-235B-A22B/snapshots/c30ce1aa8a0ff9cebf95e95b4b8fd90826043fd0/merges.txt\n",
      "loading file tokenizer.json from cache at /global/homes/j/jundac/.cache/huggingface/hub/models--Qwen--Qwen3-235B-A22B/snapshots/c30ce1aa8a0ff9cebf95e95b4b8fd90826043fd0/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /global/homes/j/jundac/.cache/huggingface/hub/models--Qwen--Qwen3-235B-A22B/snapshots/c30ce1aa8a0ff9cebf95e95b4b8fd90826043fd0/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = {}  # name → dict(start, end, params)\n",
    "\n",
    "def want_hook(name, module):\n",
    "    # leaf = no children; skip attention classes (FlashAttention2, QwenAttention, …)\n",
    "    is_leaf = len(list(module.children())) == 0\n",
    "    is_attn = \"attn\" in module.__class__.__name__.lower() or \\\n",
    "              \"attention\" in module.__class__.__name__.lower()\n",
    "    return is_leaf \n",
    "    # return is_leaf and not is_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, m in model.named_modules():\n",
    "    if not want_hook(name, m):\n",
    "        continue\n",
    "\n",
    "    # forward-pre & forward hooks share the closure variable 'name'\n",
    "    def pre_hook(mod, inp, name=name):\n",
    "        profile[name] = {\"start\": time.perf_counter(),\n",
    "                         \"params\": sum(p.numel() for p in mod.parameters())}\n",
    "\n",
    "    def post_hook(mod, inp, out, name=name):\n",
    "        torch.cuda.synchronize()\n",
    "        profile[name][\"end\"] = time.perf_counter()\n",
    "\n",
    "    m.register_forward_pre_hook(pre_hook, prepend=True)\n",
    "    m.register_forward_hook(post_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_attn_funcs = transformers.modeling_utils.ALL_ATTENTION_FUNCTIONS._global_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook the attention functions too\n",
    "new_attn_funcs = {}\n",
    "for k, v in old_attn_funcs.items():\n",
    "    def new_func(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        ret = v(*args, **kwargs)\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        profile[\"model.layers.0.self_attn.attn\"] = {\n",
    "            \"start\": start,\n",
    "            \"params\": 0,\n",
    "            \"end\": end,\n",
    "        }\n",
    "        return ret\n",
    "    new_attn_funcs[k] = new_func\n",
    "\n",
    "\n",
    "transformers.modeling_utils.ALL_ATTENTION_FUNCTIONS._global_mapping = new_attn_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module                                         latency  params\n",
      "----------------------------------------------------------------------\n",
      "model.embed_tokens                               0.248 ms   622.33 M\n",
      "model.rotary_emb                                 0.352 ms     0.00 M\n",
      "model.layers.0.input_layernorm                   0.496 ms     0.00 M\n",
      "model.layers.0.self_attn.q_proj                  1.172 ms    33.55 M\n",
      "model.layers.0.self_attn.q_norm                  0.935 ms     0.00 M\n",
      "model.layers.0.self_attn.k_proj                  0.147 ms     2.10 M\n",
      "model.layers.0.self_attn.k_norm                  0.164 ms     0.00 M\n",
      "model.layers.0.self_attn.v_proj                  0.148 ms     2.10 M\n",
      "model.layers.0.self_attn.attn                    2.956 ms     0.00 M\n",
      "model.layers.0.self_attn.o_proj                  1.143 ms    33.55 M\n",
      "model.layers.0.post_attention_layernorm          0.513 ms     0.00 M\n",
      "model.layers.0.mlp.gate                          0.085 ms     0.52 M\n",
      "model.norm                                       0.512 ms     0.00 M\n",
      "lm_head                                         19.669 ms   622.33 M\n",
      "model.layers.0.*expert*                         22.208 ms  2415.92 M\n",
      "----------------------------------------------------------------------\n",
      "ctx_len: 4096\n",
      "total layer 0 time: 29.967 ms\n",
      "full forward time: 80.201 ms\n"
     ]
    }
   ],
   "source": [
    "K = 1024\n",
    "ctx_len = K * 4\n",
    "inp = tok(\" a\" * ctx_len, return_tensors=\"pt\").to(model.device)\n",
    "ctx_len = len(inp.input_ids[0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(**inp)                        # warm-up (CUDA kernels/JIT)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    model(**inp)\n",
    "    torch.cuda.synchronize()\n",
    "    total = time.perf_counter() - start\n",
    "    total *= 1e3\n",
    "\n",
    "\n",
    "rows = []\n",
    "for name, rec in profile.items():\n",
    "    dur_ms = (rec[\"end\"] - rec[\"start\"]) * 1e3\n",
    "    rows.append((dur_ms, name, rec[\"params\"]))\n",
    "# rows.sort(reverse=True)                # slowest first\n",
    "\n",
    "print(f\"{'module':45}  latency  params\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "total_layer0_time = 0\n",
    "expert_time = 0\n",
    "expert_params = 0\n",
    "\n",
    "layer_attn_time = 0\n",
    "for dur, name, nparam in rows:\n",
    "    # if \"layers.0\" not in name:\n",
    "    #     continue\n",
    "    if \"experts\" in name:\n",
    "        expert_time += dur\n",
    "        expert_params += nparam\n",
    "    else:\n",
    "        print(f\"{name:45}  {dur:7.3f} ms  {nparam/1e6:7.2f} M\")\n",
    "    \n",
    "    if name == \"model.layers.0.self_attn.attn\":\n",
    "        layer_attn_time += dur\n",
    "\n",
    "    if \"layers.0\" not in name:\n",
    "        continue\n",
    "\n",
    "    total_layer0_time += dur\n",
    "\n",
    "print(f\"{'model.layers.0.*expert*':45}  {expert_time:7.3f} ms  {expert_params/1e6:7.2f} M\")\n",
    "print(\"-\"*70)\n",
    "print(f\"ctx_len: {ctx_len}\")\n",
    "print(f\"total layer 0 time: {total_layer0_time:.3f} ms\")\n",
    "print(f\"layer 0 attn time: {layer_attn_time:.3f} ms\")\n",
    "print(f\"full forward time: {total:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "module                                         latency  params\n",
    "----------------------------------------------------------------------\n",
    "model.layers.0.input_layernorm                   0.495 ms     0.00 M\n",
    "model.layers.0.self_attn.q_proj                  1.176 ms    33.55 M\n",
    "model.layers.0.self_attn.q_norm                  0.936 ms     0.00 M\n",
    "model.layers.0.self_attn.k_proj                  0.146 ms     2.10 M\n",
    "model.layers.0.self_attn.k_norm                  0.158 ms     0.00 M\n",
    "model.layers.0.self_attn.v_proj                  0.145 ms     2.10 M\n",
    "model.layers.0.self_attn.o_proj                  3.853 ms    33.55 M\n",
    "model.layers.0.post_attention_layernorm          0.510 ms     0.00 M\n",
    "model.layers.0.mlp.gate                          0.085 ms     0.52 M\n",
    "model.layers.0.*expert*                         23.759 ms  2415.92 M\n",
    "----------------------------------------------------------------------\n",
    "ctx_len: 4096\n",
    "total layer 0 time: 31.263 ms\n",
    "full forward time: 83.191 ms\n",
    "\n",
    "\n",
    "\n",
    "module                                         latency  params\n",
    "----------------------------------------------------------------------\n",
    "model.layers.0.input_layernorm                   0.933 ms     0.00 M\n",
    "model.layers.0.self_attn.q_proj                  2.200 ms    33.55 M\n",
    "model.layers.0.self_attn.q_norm                  1.796 ms     0.00 M\n",
    "model.layers.0.self_attn.k_proj                  0.279 ms     2.10 M\n",
    "model.layers.0.self_attn.k_norm                  0.180 ms     0.00 M\n",
    "model.layers.0.self_attn.v_proj                  0.272 ms     2.10 M\n",
    "model.layers.0.self_attn.o_proj                 10.788 ms    33.55 M\n",
    "model.layers.0.post_attention_layernorm          0.993 ms     0.00 M\n",
    "model.layers.0.mlp.gate                          0.113 ms     0.52 M\n",
    "model.layers.0.*expert*                         31.005 ms  2415.92 M\n",
    "----------------------------------------------------------------------\n",
    "ctx_len: 8192 (8K)\n",
    "total layer 0 time: 48.558 ms\n",
    "full forward time: 123.544 ms\n",
    "\n",
    "\n",
    "module                                         latency  params\n",
    "----------------------------------------------------------------------\n",
    "model.layers.0.input_layernorm                   1.755 ms     0.00 M\n",
    "model.layers.0.self_attn.q_proj                  4.331 ms    33.55 M\n",
    "model.layers.0.self_attn.q_norm                  3.517 ms     0.00 M\n",
    "model.layers.0.self_attn.k_proj                  0.392 ms     2.10 M\n",
    "model.layers.0.self_attn.k_norm                  0.289 ms     0.00 M\n",
    "model.layers.0.self_attn.v_proj                  0.385 ms     2.10 M\n",
    "model.layers.0.self_attn.o_proj                 32.830 ms    33.55 M\n",
    "model.layers.0.post_attention_layernorm          1.951 ms     0.00 M\n",
    "model.layers.0.mlp.gate                          0.163 ms     0.52 M\n",
    "model.layers.0.*expert*                         41.588 ms  2415.92 M\n",
    "----------------------------------------------------------------------\n",
    "ctx_len: 16384 (16K)\n",
    "total layer 0 time: 87.201 ms\n",
    "full forward time: 205.412 ms\n",
    "\n",
    "\n",
    "\n",
    "module                                         latency  params\n",
    "----------------------------------------------------------------------\n",
    "model.layers.0.input_layernorm                   3.340 ms     0.00 M\n",
    "model.layers.0.self_attn.q_proj                  7.718 ms    33.55 M\n",
    "model.layers.0.self_attn.q_norm                  6.709 ms     0.00 M\n",
    "model.layers.0.self_attn.k_proj                  0.563 ms     2.10 M\n",
    "model.layers.0.self_attn.k_norm                  0.483 ms     0.00 M\n",
    "model.layers.0.self_attn.v_proj                  0.559 ms     2.10 M\n",
    "model.layers.0.self_attn.o_proj                 99.912 ms    33.55 M\n",
    "model.layers.0.post_attention_layernorm          3.724 ms     0.00 M\n",
    "model.layers.0.mlp.gate                          0.323 ms     0.52 M\n",
    "model.layers.0.*expert*                         64.315 ms  2415.92 M\n",
    "----------------------------------------------------------------------\n",
    "ctx_len: 32768 (32K)\n",
    "total layer 0 time: 187.646 ms\n",
    "full forward time: 397.876 ms\n",
    "\n",
    "\n",
    "\n",
    "module                                         latency  params\n",
    "----------------------------------------------------------------------\n",
    "model.layers.0.input_layernorm                   4.983 ms     0.00 M\n",
    "model.layers.0.self_attn.q_proj                 11.568 ms    33.55 M\n",
    "model.layers.0.self_attn.q_norm                 10.093 ms     0.00 M\n",
    "model.layers.0.self_attn.k_proj                  0.886 ms     2.10 M\n",
    "model.layers.0.self_attn.k_norm                  0.692 ms     0.00 M\n",
    "model.layers.0.self_attn.v_proj                  0.855 ms     2.10 M\n",
    "model.layers.0.self_attn.o_proj                209.589 ms    33.55 M\n",
    "model.layers.0.post_attention_layernorm          5.603 ms     0.00 M\n",
    "model.layers.0.mlp.gate                          0.317 ms     0.52 M\n",
    "model.layers.0.*expert*                         86.480 ms  2415.92 M\n",
    "----------------------------------------------------------------------\n",
    "ctx_len: 49152 (48K)\n",
    "total layer 0 time: 331.066 ms\n",
    "full forward time: 632.409 ms\n",
    "\n",
    "\n",
    "\n",
    "module                                         latency  params\n",
    "----------------------------------------------------------------------\n",
    "model.layers.0.input_layernorm                   6.600 ms     0.00 M\n",
    "model.layers.0.self_attn.q_proj                 15.371 ms    33.55 M\n",
    "model.layers.0.self_attn.q_norm                 13.356 ms     0.00 M\n",
    "model.layers.0.self_attn.k_proj                  1.065 ms     2.10 M\n",
    "model.layers.0.self_attn.k_norm                  0.903 ms     0.00 M\n",
    "model.layers.0.self_attn.v_proj                  1.059 ms     2.10 M\n",
    "model.layers.0.self_attn.o_proj                359.716 ms    33.55 M\n",
    "model.layers.0.post_attention_layernorm          7.454 ms     0.00 M\n",
    "model.layers.0.mlp.gate                          0.425 ms     0.52 M\n",
    "model.layers.0.*expert*                        110.195 ms  2415.92 M\n",
    "----------------------------------------------------------------------\n",
    "ctx_len: 65536 (64K)\n",
    "total layer 0 time: 516.144 ms\n",
    "full forward time: 908.428 ms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "module                                         latency  params\n",
    "----------------------------------------------------------------------\n",
    "model.layers.0.input_layernorm                   9.858 ms     0.00 M\n",
    "model.layers.0.self_attn.q_proj                 23.034 ms    33.55 M\n",
    "model.layers.0.self_attn.q_norm                 20.070 ms     0.00 M\n",
    "model.layers.0.self_attn.k_proj                  1.570 ms     2.10 M\n",
    "model.layers.0.self_attn.k_norm                  1.328 ms     0.00 M\n",
    "model.layers.0.self_attn.v_proj                  1.565 ms     2.10 M\n",
    "model.layers.0.self_attn.o_proj                786.287 ms    33.55 M\n",
    "model.layers.0.post_attention_layernorm         11.157 ms     0.00 M\n",
    "model.layers.0.mlp.gate                          0.617 ms     0.52 M\n",
    "model.layers.0.*expert*                        155.536 ms  2415.92 M\n",
    "----------------------------------------------------------------------\n",
    "ctx_len: 98304 (96K)\n",
    "total layer 0 time: 1011.022 ms\n",
    "full forward time: 1587.503 ms\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 31.263,\n",
       " 8: 48.558,\n",
       " 16: 87.201,\n",
       " 32: 187.646,\n",
       " 48: 331.066,\n",
       " 64: 516.144,\n",
       " 96: 1011.022}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_forward_time = {\n",
    "    4: 31.263,\n",
    "    8: 48.558,\n",
    "    16: 87.201,\n",
    "    32: 187.646,\n",
    "    48: 331.066,\n",
    "    64: 516.144,\n",
    "    96: 1011.022,\n",
    "}\n",
    "mlp_forward_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module                                         latency  params\n",
      "----------------------------------------------------------------------\n",
      "model.embed_tokens                               0.248 ms   622.33 M\n",
      "model.rotary_emb                                 0.352 ms     0.00 M\n",
      "model.layers.0.input_layernorm                   0.496 ms     0.00 M\n",
      "model.layers.0.self_attn.q_proj                  1.172 ms    33.55 M\n",
      "model.layers.0.self_attn.q_norm                  0.935 ms     0.00 M\n",
      "model.layers.0.self_attn.k_proj                  0.147 ms     2.10 M\n",
      "model.layers.0.self_attn.k_norm                  0.164 ms     0.00 M\n",
      "model.layers.0.self_attn.v_proj                  0.148 ms     2.10 M\n",
      "model.layers.0.self_attn.attn                    2.956 ms     0.00 M\n",
      "model.layers.0.self_attn.o_proj                  1.143 ms    33.55 M\n",
      "model.layers.0.post_attention_layernorm          0.513 ms     0.00 M\n",
      "model.layers.0.mlp.gate                          0.085 ms     0.52 M\n",
      "model.norm                                       0.512 ms     0.00 M\n",
      "lm_head                                         19.669 ms   622.33 M\n",
      "model.layers.0.*expert*                         22.208 ms  2415.92 M\n",
      "----------------------------------------------------------------------\n",
      "full forward time: 80.201 ms\n",
      "full document forward time: 50.748 ms\n",
      "undocumented forward time: 29.453 ms\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for name, rec in profile.items():\n",
    "    dur_ms = (rec[\"end\"] - rec[\"start\"]) * 1e3\n",
    "    rows.append((dur_ms, name, rec[\"params\"]))\n",
    "# rows.sort(reverse=True)                # slowest first\n",
    "\n",
    "print(f\"{'module':45}  latency  params\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "total_doc_time = 0\n",
    "expert_time = 0\n",
    "expert_params = 0\n",
    "for dur, name, nparam in rows:\n",
    "    if \"experts\" in name:\n",
    "        expert_time += dur\n",
    "        expert_params += nparam\n",
    "    else:\n",
    "        print(f\"{name:45}  {dur:7.3f} ms  {nparam/1e6:7.2f} M\")\n",
    "        pass\n",
    "    total_doc_time += dur\n",
    "print(f\"{'model.layers.0.*expert*':45}  {expert_time:7.3f} ms  {expert_params/1e6:7.2f} M\")\n",
    "print(\"-\"*70)\n",
    "print(f\"full forward time: {total:.3f} ms\")\n",
    "print(f\"full document forward time: {total_doc_time:.3f} ms\")\n",
    "print(f\"undocumented forward time: {total - total_doc_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash_attn\n",
    "from flash_attn.flash_attn_interface import flash_attn_varlen_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qo_heads = cfg.num_attention_heads\n",
    "num_kv_heads = cfg.num_key_value_heads\n",
    "head_dim = cfg.hidden_size // num_qo_heads\n",
    "tp = 1\n",
    "cp = 1\n",
    "device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [16 * 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average latency: 12.630 ms\n"
     ]
    }
   ],
   "source": [
    "# Qwen3 253B activate attention data\n",
    "num_qo_heads = num_qo_heads // tp\n",
    "num_kv_heads = max(num_kv_heads // tp, 1)\n",
    "\n",
    "kv_lens = [(1/2 + 1/(2 * cp)) * i for i in batch]\n",
    "\n",
    "batch = [int(i // cp) for i in batch]\n",
    "kv_lens = [int(i) for i in kv_lens]\n",
    "\n",
    "total_tokens = sum(batch)\n",
    "total_kv_tokens = sum(kv_lens)\n",
    "\n",
    "q = torch.randn(total_tokens, num_qo_heads, head_dim, device=device, dtype=torch.bfloat16)\n",
    "k = torch.randn(total_kv_tokens, num_kv_heads, head_dim, device=device, dtype=torch.bfloat16)\n",
    "v = torch.randn(total_kv_tokens, num_kv_heads, head_dim, device=device, dtype=torch.bfloat16)\n",
    "max_seqlen_q = max(batch)\n",
    "max_seqlen_k = max(kv_lens)\n",
    "\n",
    "cu_seqlens_q = [0,]\n",
    "cu_seqlens_k = [0,]\n",
    "for idx, _ in enumerate(batch):\n",
    "    cu_seqlens_q.append(sum(batch[:idx+1]))\n",
    "    cu_seqlens_k.append(sum(kv_lens[:idx+1]))\n",
    "cu_seqlens_q = torch.tensor(cu_seqlens_q, dtype=torch.int32, device=device)\n",
    "cu_seqlens_k = torch.tensor(cu_seqlens_k, dtype=torch.int32, device=device)\n",
    "max_seqlen_q = torch.tensor(max_seqlen_q, dtype=torch.int32, device=device)\n",
    "max_seqlen_k = torch.tensor(max_seqlen_k, dtype=torch.int32, device=device)\n",
    "\n",
    "\n",
    "def test_flash_attn():\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "    start_event.record()\n",
    "    output = flash_attn_varlen_func(\n",
    "        q, k, v, \n",
    "        cu_seqlens_q, cu_seqlens_k, \n",
    "        max_seqlen_q, max_seqlen_k,\n",
    "        dropout_p=0.0, causal=True,\n",
    "    )\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "    duration = start_event.elapsed_time(end_event)\n",
    "    return duration\n",
    "\n",
    "# warmup\n",
    "for _ in range(5):\n",
    "    test_flash_attn()\n",
    "\n",
    "# benchmark\n",
    "num_iters = 10\n",
    "durations = []\n",
    "for _ in range(num_iters):\n",
    "    duration = test_flash_attn()\n",
    "    durations.append(duration)\n",
    "\n",
    "avg_duration = sum(durations) / len(durations)\n",
    "print(f\"average latency: {avg_duration:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
