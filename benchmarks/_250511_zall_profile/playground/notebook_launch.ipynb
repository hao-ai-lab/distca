{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(f\"Starting notebook: {time.time()}\")\n",
    "from accelerate import notebook_launcher\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_record = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "[rank 1] Running on rank 1\n",
      "[rank 0] Running on rank 0\n",
      "[rank 2] Running on rank 2\n",
      "[rank 3] Running on rank 3\n",
      "[rank 1] fw_time=186.25 ± 48.66, bw_time=667.63 ± 66.66\n",
      "[rank 0] fw_time=190.75 ± 47.66, bw_time=699.66 ± 100.15\n",
      "[rank 2] fw_time=174.03 ± 37.51, bw_time=711.10 ± 228.03\n",
      "[rank 3] fw_time=208.06 ± 59.54, bw_time=674.61 ± 274.36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_fn():\n",
    "    start_time = time.time()\n",
    "    # print(f\"Initializing process group: {time.time()}\")\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    end_time = time.time()\n",
    "    rank = torch.distributed.get_rank()\n",
    "    def print_rank(message):\n",
    "        print(f\"[rank {rank}] {message}\")\n",
    "    # print_rank(f\"Time taken to initialize process group: {end_time - start_time} seconds\")\n",
    "    print_rank(f\"Running on rank {torch.distributed.get_rank()}\")\n",
    "\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    # Start benchmarking linear layers with TP.\n",
    "    import torch.nn as nn\n",
    "    tp = torch.distributed.get_world_size()\n",
    "    head_dim = 128\n",
    "    num_qo_heads = 32 // tp\n",
    "    num_kv_heads = 8 // tp\n",
    "\n",
    "    hidden_dim = head_dim * num_qo_heads\n",
    "    l1 = nn.Linear(hidden_dim, hidden_dim * 4, bias=False)\n",
    "    # print(l1.weight.shape)\n",
    "    \n",
    "    # test the forward and backward time of l1\n",
    "    K = 1024\n",
    "    ctx_length = 16 * K\n",
    "    x = torch.randn(ctx_length, hidden_dim).requires_grad_(True)\n",
    "    z = torch.randn(ctx_length, hidden_dim * 4).requires_grad_(False)\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    fw_times = []\n",
    "    bw_times = []\n",
    "\n",
    "    for _ in range(10):\n",
    "        fw_start_evt = torch.cuda.Event(enable_timing=True)\n",
    "        fw_end_evt = torch.cuda.Event(enable_timing=True)\n",
    "        bw_start_evt = torch.cuda.Event(enable_timing=True)\n",
    "        bw_end_evt = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        fw_start_evt.record()\n",
    "        y = l1(x)\n",
    "        fw_end_evt.record()\n",
    "\n",
    "        bw_start_evt.record()\n",
    "        y.backward(z)\n",
    "        bw_end_evt.record()\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        fw_time = fw_start_evt.elapsed_time(fw_end_evt)\n",
    "        bw_time = bw_start_evt.elapsed_time(bw_end_evt)\n",
    "        \n",
    "        fw_times.append(fw_time)\n",
    "        bw_times.append(bw_time)\n",
    "\n",
    "    fw_avg = np.mean(fw_times)\n",
    "    fw_std = np.std(fw_times)\n",
    "    bw_avg = np.mean(bw_times)\n",
    "    bw_std = np.std(bw_times)\n",
    "\n",
    "    print_rank(f\"fw_time={fw_avg:.2f} ± {fw_std:.2f}, bw_time={bw_avg:.2f} ± {bw_std:.2f}\")\n",
    "\n",
    "\n",
    "# print(f\"Starting notebook launcher: {time.time()}\")\n",
    "notebook_launcher(train_fn, num_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.profiler\n",
    "\n",
    "def profile_linear(\n",
    "    device: int = 0,\n",
    "    input_dim: int = 128 * (32 // 1),\n",
    "    output_dim: int = 128 * (32 // 1) * 4,\n",
    "    seq_len: int = 1024,\n",
    "    warmup: int = 2,\n",
    "    active: int = 10,\n",
    "    log_dir: str = \"./profiler_logs/single_gpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Profiles a single nn.Linear(hidden_dim, hidden_dim*4) forward+backward on one GPU.\n",
    "\n",
    "    Args:\n",
    "      device:       CUDA device index.\n",
    "      hidden_dim:   Input feature size.\n",
    "      seq_len:      Sequence length (number of rows).\n",
    "      warmup:       Number of warm‑up iterations (unrecorded).\n",
    "      active:       Number of profiled iterations.\n",
    "      log_dir:      Directory where TensorBoard/Chrome‑trace logs go.\n",
    "    \"\"\"\n",
    "    # 1) Set up device & model\n",
    "    torch.cuda.set_device(device)\n",
    "    model = torch.compile(nn.Linear(input_dim, output_dim, bias=False, device=device))\n",
    "\n",
    "    # 2) Dummy inputs\n",
    "    x = torch.randn(seq_len, input_dim, device=device, requires_grad=True)\n",
    "    z = torch.randn(seq_len, output_dim, device=device)\n",
    "\n",
    "    # 3) Profiler schedule & handler\n",
    "    schedule = torch.profiler.schedule(\n",
    "        wait=warmup,\n",
    "        warmup=warmup,\n",
    "        active=active,\n",
    "        repeat=1,\n",
    "    )\n",
    "    tb_handler = torch.profiler.tensorboard_trace_handler(\n",
    "        dir_name=log_dir\n",
    "    )\n",
    "    from torch.cuda import nvtx\n",
    "\n",
    "    fw_times = []\n",
    "    bw_times = []\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=tb_handler,\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True,\n",
    "        with_flops=True,\n",
    "        # use_cuda=True,\n",
    "    ) as prof:\n",
    "        total_steps = warmup + active\n",
    "        for step in range(total_steps):\n",
    "            # Use NVTX for marking regions\n",
    "            nvtx.range_push(f\"Step {step}\")\n",
    "\n",
    "            # Timing for forward pass\n",
    "            forward_start_event = torch.cuda.Event(enable_timing=True)\n",
    "            forward_end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "            forward_start_event.record()\n",
    "            nvtx.range_push(\"Forward Pass\")\n",
    "            y = model(x)\n",
    "            nvtx.range_pop()\n",
    "            forward_end_event.record()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            forward_time = forward_start_event.elapsed_time(forward_end_event)\n",
    "\n",
    "            # Timing for backward pass\n",
    "            backward_start_event = torch.cuda.Event(enable_timing=True)\n",
    "            backward_end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "            backward_start_event.record()\n",
    "            nvtx.range_push(\"Backward Pass\")\n",
    "            y.backward(z)\n",
    "            nvtx.range_pop()\n",
    "            backward_end_event.record()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            backward_time = backward_start_event.elapsed_time(backward_end_event)\n",
    "\n",
    "            # if step == warmup - 1:\n",
    "            #     print(f\"[device {device}] warm‑up done, starting profiling\")\n",
    "            # if (step >= warmup):\n",
    "            #     total_time = forward_time + backward_time\n",
    "            #     print(f\"[device {device}] prof step {step-warmup}/{active}, forward: {forward_time:.2f} ms, backward: {backward_time:.2f} ms, total: {total_time:.2f} ms\")\n",
    "\n",
    "            nvtx.range_pop()\n",
    "            prof.step()\n",
    "            if (step >= warmup):\n",
    "                fw_times.append(forward_time)\n",
    "                bw_times.append(backward_time)\n",
    "    \n",
    "    # print(f\"[device {device}] profiling complete. Logs in: {log_dir}\")\n",
    "    return fw_times, bw_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1117850/4093237169.py:49: FutureWarning: `use_cuda` is deprecated, use `activities` argument instead\n",
      "  with torch.profiler.profile(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len=2 ** 1K, fw_time=14.80 ± 0.51, bw_time=29.52 ± 0.07\n",
      "seq_len=2 ** 2K, fw_time=29.15 ± 0.12, bw_time=58.10 ± 0.19\n",
      "seq_len=2 ** 3K, fw_time=58.08 ± 0.10, bw_time=115.63 ± 0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1117850/4093237169.py:49: FutureWarning: `use_cuda` is deprecated, use `activities` argument instead\n",
      "  with torch.profiler.profile(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len=2 ** 4K, fw_time=116.00 ± 0.16, bw_time=232.47 ± 0.13\n",
      "seq_len=2 ** 5K, fw_time=231.76 ± 0.15, bw_time=461.06 ± 0.21\n",
      "seq_len=2 ** 6K, fw_time=462.61 ± 0.21, bw_time=920.40 ± 0.21\n",
      "seq_len=2 ** 7K, fw_time=924.86 ± 0.49, bw_time=1838.88 ± 0.35\n",
      "seq_len=2 ** 8K, fw_time=1851.89 ± 0.14, bw_time=3755.85 ± 1.12\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 30.35 GiB is free. Including non-PyTorch memory, this process has 48.78 GiB memory in use. Of the allocated memory 40.27 GiB is allocated by PyTorch, and 8.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[110]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seq_len_factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m):\n\u001b[32m     11\u001b[39m     seq_len = K * (\u001b[32m2\u001b[39m ** seq_len_factor)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     fw_times, bw_times = profile_linear(\n\u001b[32m     13\u001b[39m         device=\u001b[32m0\u001b[39m,\n\u001b[32m     14\u001b[39m         input_dim=\u001b[32m128\u001b[39m * (\u001b[32m32\u001b[39m // tp),\n\u001b[32m     15\u001b[39m         output_dim=\u001b[32m128\u001b[39m * (\u001b[32m32\u001b[39m // tp) * \u001b[32m4\u001b[39m,\n\u001b[32m     16\u001b[39m         seq_len=seq_len,\n\u001b[32m     17\u001b[39m         \u001b[38;5;66;03m# warmup=10,\u001b[39;00m\n\u001b[32m     18\u001b[39m         warmup=\u001b[32m5\u001b[39m,\n\u001b[32m     19\u001b[39m         active=\u001b[32m8\u001b[39m,\n\u001b[32m     20\u001b[39m         log_dir=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./my_logs_seq_len_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     )\n\u001b[32m     22\u001b[39m     all_fw_times.append(fw_times)\n\u001b[32m     23\u001b[39m     all_bw_times.append(bw_times)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mprofile_linear\u001b[39m\u001b[34m(device, input_dim, output_dim, seq_len, warmup, active, log_dir)\u001b[39m\n\u001b[32m     67\u001b[39m forward_start_event.record()\n\u001b[32m     68\u001b[39m nvtx.range_push(\u001b[33m\"\u001b[39m\u001b[33mForward Pass\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m y = model(x)\n\u001b[32m     70\u001b[39m nvtx.range_pop()\n\u001b[32m     71\u001b[39m forward_end_event.record()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:574\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m saved_dynamic_layer_stack_depth = (\n\u001b[32m    570\u001b[39m     torch._C._functorch.get_dynamic_layer_stack_depth()\n\u001b[32m    571\u001b[39m )\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    576\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[32m    577\u001b[39m     torch._C._functorch.pop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[32m    578\u001b[39m         saved_dynamic_layer_stack_depth\n\u001b[32m    579\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/_dynamo/external_utils.py:43\u001b[39m, in \u001b[36mwrap_inline.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_inline\u001b[39m(fn: Callable[..., Any]) -> Callable[..., Any]:\n\u001b[32m     39\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    Create an extra frame around fn that is not in skipfiles.\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:1184\u001b[39m, in \u001b[36maot_module_simplified.<locals>.forward\u001b[39m\u001b[34m(*runtime_args)\u001b[39m\n\u001b[32m   1182\u001b[39m full_args.extend(params_flat)\n\u001b[32m   1183\u001b[39m full_args.extend(runtime_args)\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(full_args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:310\u001b[39m, in \u001b[36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    304\u001b[39m     \u001b[38;5;66;03m# It's possible to have trace_joint inside user specified with no_grad() region,\u001b[39;00m\n\u001b[32m    305\u001b[39m     \u001b[38;5;66;03m# if there is a nested with enable_grad(), that forces some outputs to require gradients.\u001b[39;00m\n\u001b[32m    306\u001b[39m     \u001b[38;5;66;03m# Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.\u001b[39;00m\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd._force_original_view_tracking(\n\u001b[32m    308\u001b[39m         \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    309\u001b[39m     ), torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m         all_outs = call_func_at_runtime_with_args(\n\u001b[32m    311\u001b[39m             compiled_fn, args_, disable_amp=disable_amp, steal_args=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    312\u001b[39m         )\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    314\u001b[39m     \u001b[38;5;66;03m# When we have an inference graph, we run with grad disabled.\u001b[39;00m\n\u001b[32m    315\u001b[39m     \u001b[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001b[39;00m\n\u001b[32m    316\u001b[39m     \u001b[38;5;66;03m# in which case we want to make sure autograd is disabled\u001b[39;00m\n\u001b[32m    317\u001b[39m     \u001b[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[39;00m\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# NOTE: We use _set_grad_enabled directly to reduce runtime overhead\u001b[39;00m\n\u001b[32m    319\u001b[39m     grad_enabled = torch.is_grad_enabled()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(f(args))\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:100\u001b[39m, in \u001b[36mmake_boxed_func.<locals>.g\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mg\u001b[39m(args):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(*args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().apply(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1585\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.forward\u001b[39m\u001b[34m(ctx, *deduped_flat_tensor_args)\u001b[39m\n\u001b[32m   1576\u001b[39m     ctx._compiled_autograd_backward_state = bw_state\n\u001b[32m   1578\u001b[39m \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[32m   1579\u001b[39m \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[32m   1580\u001b[39m \u001b[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1583\u001b[39m \u001b[38;5;66;03m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[39;00m\n\u001b[32m   1584\u001b[39m \u001b[38;5;66;03m#   in the fw output order.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1585\u001b[39m fw_outs = call_func_at_runtime_with_args(\n\u001b[32m   1586\u001b[39m     CompiledFunction.compiled_fw,\n\u001b[32m   1587\u001b[39m     args,\n\u001b[32m   1588\u001b[39m     disable_amp=disable_amp,\n\u001b[32m   1589\u001b[39m )\n\u001b[32m   1591\u001b[39m num_outputs = CompiledFunction.metadata.num_outputs\n\u001b[32m   1592\u001b[39m num_outputs_aliased = CompiledFunction.metadata.num_outputs_aliased\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(f(args))\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:490\u001b[39m, in \u001b[36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[39m\u001b[34m(runtime_args)\u001b[39m\n\u001b[32m    483\u001b[39m     out = \u001b[38;5;28mself\u001b[39m._functionalized_rng_runtime_epilogue(\n\u001b[32m    484\u001b[39m         runtime_metadata,\n\u001b[32m    485\u001b[39m         out,\n\u001b[32m    486\u001b[39m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[32m    487\u001b[39m         runtime_metadata.num_forward_returns,\n\u001b[32m    488\u001b[39m     )\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(runtime_args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:672\u001b[39m, in \u001b[36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    669\u001b[39m     args = [*([\u001b[38;5;28;01mNone\u001b[39;00m] * num_tokens), *args]\n\u001b[32m    670\u001b[39m     old_args.clear()\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m outs = compiled_fn(args)\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/_inductor/output_code.py:466\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable(inputs)\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    468\u001b[39m     AutotuneCacheBundler.end_compile()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envs/d2/lib/python3.12/site-packages/torch/_inductor/utils.py:2128\u001b[39m, in \u001b[36malign_inputs_from_check_idxs.<locals>.run\u001b[39m\u001b[34m(new_inputs)\u001b[39m\n\u001b[32m   2126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(new_inputs: List[InputType]):\n\u001b[32m   2127\u001b[39m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[32m-> \u001b[39m\u001b[32m2128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model(new_inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/torchinductor_jundac/vv/cvvnj3byisqptzkwxo5o7qp5pgn3fe2nrdip7p5swb7hrwyee73y.py:41\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda._DeviceGuard(\u001b[32m0\u001b[39m):\n\u001b[32m     40\u001b[39m     torch.cuda.set_device(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     buf0 = empty_strided_cuda((s0, \u001b[32m16384\u001b[39m), (\u001b[32m16384\u001b[39m, \u001b[32m1\u001b[39m), torch.float32)\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [linear], Original ATen: [aten.mm]\u001b[39;00m\n\u001b[32m     43\u001b[39m     extern_kernels.mm(primals_3, reinterpret_tensor(primals_1, (\u001b[32m4096\u001b[39m, \u001b[32m16384\u001b[39m), (\u001b[32m1\u001b[39m, \u001b[32m4096\u001b[39m), \u001b[32m0\u001b[39m), out=buf0)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 30.35 GiB is free. Including non-PyTorch memory, this process has 48.78 GiB memory in use. Of the allocated memory 40.27 GiB is allocated by PyTorch, and 8.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store forward and backward times for each seq_len\n",
    "all_fw_times = []\n",
    "all_bw_times = []\n",
    "\n",
    "# Loop over different sequence lengths\n",
    "tp = 1\n",
    "K = 1024\n",
    "for seq_len_factor in range(1, 10):\n",
    "    seq_len = K * (2 ** seq_len_factor)\n",
    "    fw_times, bw_times = profile_linear(\n",
    "        device=0,\n",
    "        input_dim=128 * (32 // tp),\n",
    "        output_dim=128 * (32 // tp) * 4,\n",
    "        seq_len=seq_len,\n",
    "        # warmup=10,\n",
    "        warmup=5,\n",
    "        active=8,\n",
    "        log_dir=f\"./my_logs_seq_len_{seq_len}\",\n",
    "    )\n",
    "    all_fw_times.append(fw_times)\n",
    "    all_bw_times.append(bw_times)\n",
    "\n",
    "    fw_avg = np.mean(fw_times)\n",
    "    fw_std = np.std(fw_times)\n",
    "    bw_avg = np.mean(bw_times)\n",
    "    bw_std = np.std(bw_times)\n",
    "\n",
    "    print(f\"seq_len=2 ** {seq_len_factor}K, fw_time={fw_avg:.2f} ± {fw_std:.2f}, bw_time={bw_avg:.2f} ± {bw_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nseq_len=2 ** 1, fw_time=7.40 ± 0.09, bw_time=15.54 ± 0.05\\nseq_len=2 ** 2, fw_time=14.55 ± 0.07, bw_time=29.48 ± 0.04\\nseq_len=2 ** 3, fw_time=21.76 ± 0.09, bw_time=44.28 ± 0.07\\nseq_len=2 ** 4, fw_time=29.09 ± 0.08, bw_time=58.08 ± 0.06\\nseq_len=2 ** 5, fw_time=36.10 ± 0.09, bw_time=72.75 ± 0.05\\nseq_len=2 ** 6, fw_time=43.57 ± 0.08, bw_time=87.49 ± 0.06\\nseq_len=2 ** 7, fw_time=50.46 ± 0.08, bw_time=101.72 ± 0.04\\nseq_len=2 ** 8, fw_time=58.03 ± 0.08, bw_time=115.67 ± 0.07\\nseq_len=2 ** 9, fw_time=65.68 ± 0.09, bw_time=131.83 ± 0.17\\n'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "seq_len=2 ** 1K, fw_time=14.80 ± 0.51, bw_time=29.52 ± 0.07\n",
    "seq_len=2 ** 2K, fw_time=29.15 ± 0.12, bw_time=58.10 ± 0.19\n",
    "seq_len=2 ** 3K, fw_time=58.08 ± 0.10, bw_time=115.63 ± 0.09\n",
    "seq_len=2 ** 4K, fw_time=116.00 ± 0.16, bw_time=232.47 ± 0.13\n",
    "seq_len=2 ** 5K, fw_time=231.76 ± 0.15, bw_time=461.06 ± 0.21\n",
    "seq_len=2 ** 6K, fw_time=462.61 ± 0.21, bw_time=920.40 ± 0.21\n",
    "seq_len=2 ** 7K, fw_time=924.86 ± 0.49, bw_time=1838.88 ± 0.35\n",
    "seq_len=2 ** 8K, fw_time=1851.89 ± 0.14, bw_time=3755.85 ± 1.12\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
